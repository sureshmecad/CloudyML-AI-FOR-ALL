{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sureshmecad/CloudyML-AI-FOR-ALL/blob/main/SVM_CloudyML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jN-srpzE0X86"
      },
      "source": [
        "# <center><u>Support Vector Machines (SVM)</u></center>\n",
        "\n",
        "\n",
        "Support vector machine is another simple algorithm that every machine learning expert should have in his/her arsenal. Support vector machine is highly preferred by many as it produces significant accuracy with less computation power. Support Vector Machine, abbreviated as SVM can be used for both regression and classification tasks. But, it is widely used in classification objectives.\n",
        "\n",
        "\n",
        "Please do refer below youtube links which would help you to crack interviews on SVM\n",
        "\n",
        "part1: https://www.youtube.com/watch?v=H9yACitf-KM\n",
        "\n",
        "part2: https://www.youtube.com/watch?v=Js3GLb1xPhc\n",
        "\n",
        "### PROBLEM STATEMENT- Email Spam Classifier\n",
        "\n",
        "\n",
        "In this assignment, we'll build a SVM classifier to classify emails into spam and ham. The dataset, taken from the UCI ML repository, contains about 4600 emails labelled as **spam** or **ham**. \n",
        "\n",
        "The dataset can be downloaded from your canvas account\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9rgrsOK0X88"
      },
      "source": [
        "## Data Understanding\n",
        "\n",
        "Let's first load the data and understand the attributes meanings, shape of the dataset etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5ZjOey1m0X89"
      },
      "outputs": [],
      "source": [
        "'''import numpy, pandas, train_test_split, StandardScaler,confusion_matrix, \n",
        "validation_curve, KFold, cross_val_score, GridSearchCV, matplotlib and seaborn'''\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        " \n",
        "from sklearn.metrics import confusion_matrix \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import validation_curve, KFold, cross_val_score, GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwE2JWtt0r-5",
        "outputId": "6c48c10d-99be-418c-f527-0743c79bc5be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LaLH2y0t0X8-"
      },
      "outputs": [],
      "source": [
        "# load the data, use seperation as comma(,) and header as None\n",
        "email_rec = pd.read_csv(\"/content/drive/MyDrive/CloudyML/Spam.zip\", sep=',', header=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "QhkdKqkn0X8_",
        "outputId": "8d730a00-3ca1-41b2-82ff-dd1d1a0238cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-cc57e90b-a2ff-44b1-b7f0-12b35c1f422a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_freq_make</th>\n",
              "      <th>word_freq_address</th>\n",
              "      <th>word_freq_all</th>\n",
              "      <th>word_freq_3d</th>\n",
              "      <th>word_freq_our</th>\n",
              "      <th>word_freq_over</th>\n",
              "      <th>word_freq_remove</th>\n",
              "      <th>word_freq_internet</th>\n",
              "      <th>word_freq_order</th>\n",
              "      <th>word_freq_mail</th>\n",
              "      <th>word_freq_receive</th>\n",
              "      <th>word_freq_will</th>\n",
              "      <th>word_freq_people</th>\n",
              "      <th>word_freq_report</th>\n",
              "      <th>word_freq_addresses</th>\n",
              "      <th>word_freq_free</th>\n",
              "      <th>word_freq_business</th>\n",
              "      <th>word_freq_email</th>\n",
              "      <th>word_freq_you</th>\n",
              "      <th>word_freq_credit</th>\n",
              "      <th>word_freq_your</th>\n",
              "      <th>word_freq_font</th>\n",
              "      <th>word_freq_000</th>\n",
              "      <th>word_freq_money</th>\n",
              "      <th>word_freq_hp</th>\n",
              "      <th>word_freq_hpl</th>\n",
              "      <th>word_freq_george</th>\n",
              "      <th>word_freq_650</th>\n",
              "      <th>word_freq_lab</th>\n",
              "      <th>word_freq_labs</th>\n",
              "      <th>word_freq_telnet</th>\n",
              "      <th>word_freq_857</th>\n",
              "      <th>word_freq_data</th>\n",
              "      <th>word_freq_415</th>\n",
              "      <th>word_freq_85</th>\n",
              "      <th>word_freq_technology</th>\n",
              "      <th>word_freq_1999</th>\n",
              "      <th>word_freq_parts</th>\n",
              "      <th>word_freq_pm</th>\n",
              "      <th>word_freq_direct</th>\n",
              "      <th>word_freq_cs</th>\n",
              "      <th>word_freq_meeting</th>\n",
              "      <th>word_freq_original</th>\n",
              "      <th>word_freq_project</th>\n",
              "      <th>word_freq_re</th>\n",
              "      <th>word_freq_edu</th>\n",
              "      <th>word_freq_table</th>\n",
              "      <th>word_freq_conference</th>\n",
              "      <th>char_freq_;</th>\n",
              "      <th>char_freq_(</th>\n",
              "      <th>char_freq_[</th>\n",
              "      <th>char_freq_!</th>\n",
              "      <th>char_freq_$</th>\n",
              "      <th>char_freq_hash</th>\n",
              "      <th>capital_run_length_average</th>\n",
              "      <th>capital_run_length_longest</th>\n",
              "      <th>capital_run_length_total</th>\n",
              "      <th>spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.29</td>\n",
              "      <td>1.93</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.778</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.756</td>\n",
              "      <td>61</td>\n",
              "      <td>278</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.94</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.79</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.28</td>\n",
              "      <td>3.47</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.59</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.048</td>\n",
              "      <td>5.114</td>\n",
              "      <td>101</td>\n",
              "      <td>1028</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.45</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.75</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>1.03</td>\n",
              "      <td>1.36</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.16</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.010</td>\n",
              "      <td>9.821</td>\n",
              "      <td>485</td>\n",
              "      <td>2259</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4596</th>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.88</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.62</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.232</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.142</td>\n",
              "      <td>3</td>\n",
              "      <td>88</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4597</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>6.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.353</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.555</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4598</th>\n",
              "      <td>0.30</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.90</td>\n",
              "      <td>1.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.718</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.404</td>\n",
              "      <td>6</td>\n",
              "      <td>118</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4599</th>\n",
              "      <td>0.96</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.93</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.057</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.147</td>\n",
              "      <td>5</td>\n",
              "      <td>78</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4600</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4.60</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.97</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>1.250</td>\n",
              "      <td>5</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4601 rows Ã— 58 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cc57e90b-a2ff-44b1-b7f0-12b35c1f422a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cc57e90b-a2ff-44b1-b7f0-12b35c1f422a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cc57e90b-a2ff-44b1-b7f0-12b35c1f422a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      word_freq_make  word_freq_address  ...  capital_run_length_total  spam\n",
              "0               0.00               0.64  ...                       278     1\n",
              "1               0.21               0.28  ...                      1028     1\n",
              "2               0.06               0.00  ...                      2259     1\n",
              "3               0.00               0.00  ...                       191     1\n",
              "4               0.00               0.00  ...                       191     1\n",
              "...              ...                ...  ...                       ...   ...\n",
              "4596            0.31               0.00  ...                        88     0\n",
              "4597            0.00               0.00  ...                        14     0\n",
              "4598            0.30               0.00  ...                       118     0\n",
              "4599            0.96               0.00  ...                        78     0\n",
              "4600            0.00               0.00  ...                        40     0\n",
              "\n",
              "[4601 rows x 58 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# print email_rec\n",
        "email_rec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuVhIoFF0X8_"
      },
      "source": [
        "As of now, the columns are named as integers. Let's manually name the columns appropriately (column names are available below sequencially) \n",
        "\n",
        "`[\"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \n",
        "                      \"word_freq_our\", \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\", \n",
        "                      \"word_freq_order\", \"word_freq_mail\", \"word_freq_receive\", \"word_freq_will\", \n",
        "                      \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\", \"word_freq_free\", \n",
        "                      \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\", \n",
        "                      \"word_freq_your\", \"word_freq_font\", \"word_freq_000\", \"word_freq_money\", \"word_freq_hp\", \n",
        "                      \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\", \"word_freq_labs\", \n",
        "                      \"word_freq_telnet\", \"word_freq_857\", \"word_freq_data\", \"word_freq_415\", \"word_freq_85\", \n",
        "                      \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\", \"word_freq_direct\",\n",
        "                      \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\", \"word_freq_re\", \n",
        "                      \"word_freq_edu\", \"word_freq_table\", \"word_freq_conference\", \"char_freq_;\", \"char_freq_(\", \n",
        "                      \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_hash\", \"capital_run_length_average\", \n",
        "                      \"capital_run_length_longest\", \"capital_run_length_total\", \"spam\"]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qOBX80bi0X9A",
        "outputId": "4498bb91-ba26-4be1-9e0e-184fdd881ce4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   word_freq_make  word_freq_address  ...  capital_run_length_total  spam\n",
            "0            0.00               0.64  ...                       278     1\n",
            "1            0.21               0.28  ...                      1028     1\n",
            "2            0.06               0.00  ...                      2259     1\n",
            "3            0.00               0.00  ...                       191     1\n",
            "4            0.00               0.00  ...                       191     1\n",
            "\n",
            "[5 rows x 58 columns]\n"
          ]
        }
      ],
      "source": [
        "# renaming the columns\n",
        "#email_rec.columns = [\"word_freq_make\", \"word_freq_address\", \"word_freq_all\", \"word_freq_3d\", \"word_freq_our\", \"word_freq_over\", \"word_freq_remove\", \"word_freq_internet\",\n",
        "#                      \"word_freq_order\", \"word_freq_mail\", \"word_freq_receive\", \"word_freq_will\", \"word_freq_people\", \"word_freq_report\", \"word_freq_addresses\",\n",
        "#                      \"word_freq_free\", \"word_freq_business\", \"word_freq_email\", \"word_freq_you\", \"word_freq_credit\", \"word_freq_your\", \"word_freq_font\", \"word_freq_000\",\n",
        "#                      \"word_freq_money\", \"word_freq_hp\", \"word_freq_hpl\", \"word_freq_george\", \"word_freq_650\", \"word_freq_lab\", \"word_freq_labs\", \"word_freq_telnet\",\n",
        "#                      \"word_freq_857\", \"word_freq_data\", \"word_freq_415\", \"word_freq_85\", \"word_freq_technology\", \"word_freq_1999\", \"word_freq_parts\", \"word_freq_pm\",\n",
        "#                      \"word_freq_direct\", \"word_freq_cs\", \"word_freq_meeting\", \"word_freq_original\", \"word_freq_project\", \"word_freq_re\", \"word_freq_edu\", \"word_freq_table\",\n",
        "#                      \"word_freq_conference\", \"char_freq_;\", \"char_freq_(\", \"char_freq_[\", \"char_freq_!\", \"char_freq_$\", \"char_freq_hash\", \"capital_run_length_average\",\n",
        "#                      \"capital_run_length_longest\", \"capital_run_length_total\", \"spam\"]\n",
        "\n",
        "# print head\n",
        "print(email_rec.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cl-jm-xO0X9A",
        "outputId": "771826b5-d0ee-4c22-edd5-648f62c895eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4601, 58)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# look at dimensions of the DataFrame\n",
        "email_rec.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYWunsIl0X9A",
        "outputId": "7698dfac-0b7f-4d9a-a987-8a9ee430ad96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4601 entries, 0 to 4600\n",
            "Data columns (total 58 columns):\n",
            " #   Column                      Non-Null Count  Dtype  \n",
            "---  ------                      --------------  -----  \n",
            " 0   word_freq_make              4601 non-null   float64\n",
            " 1   word_freq_address           4601 non-null   float64\n",
            " 2   word_freq_all               4601 non-null   float64\n",
            " 3   word_freq_3d                4601 non-null   float64\n",
            " 4   word_freq_our               4601 non-null   float64\n",
            " 5   word_freq_over              4601 non-null   float64\n",
            " 6   word_freq_remove            4601 non-null   float64\n",
            " 7   word_freq_internet          4601 non-null   float64\n",
            " 8   word_freq_order             4601 non-null   float64\n",
            " 9   word_freq_mail              4601 non-null   float64\n",
            " 10  word_freq_receive           4601 non-null   float64\n",
            " 11  word_freq_will              4601 non-null   float64\n",
            " 12  word_freq_people            4601 non-null   float64\n",
            " 13  word_freq_report            4601 non-null   float64\n",
            " 14  word_freq_addresses         4601 non-null   float64\n",
            " 15  word_freq_free              4601 non-null   float64\n",
            " 16  word_freq_business          4601 non-null   float64\n",
            " 17  word_freq_email             4601 non-null   float64\n",
            " 18  word_freq_you               4601 non-null   float64\n",
            " 19  word_freq_credit            4601 non-null   float64\n",
            " 20  word_freq_your              4601 non-null   float64\n",
            " 21  word_freq_font              4601 non-null   float64\n",
            " 22  word_freq_000               4601 non-null   float64\n",
            " 23  word_freq_money             4601 non-null   float64\n",
            " 24  word_freq_hp                4601 non-null   float64\n",
            " 25  word_freq_hpl               4601 non-null   float64\n",
            " 26  word_freq_george            4601 non-null   float64\n",
            " 27  word_freq_650               4601 non-null   float64\n",
            " 28  word_freq_lab               4601 non-null   float64\n",
            " 29  word_freq_labs              4601 non-null   float64\n",
            " 30  word_freq_telnet            4601 non-null   float64\n",
            " 31  word_freq_857               4601 non-null   float64\n",
            " 32  word_freq_data              4601 non-null   float64\n",
            " 33  word_freq_415               4601 non-null   float64\n",
            " 34  word_freq_85                4601 non-null   float64\n",
            " 35  word_freq_technology        4601 non-null   float64\n",
            " 36  word_freq_1999              4601 non-null   float64\n",
            " 37  word_freq_parts             4601 non-null   float64\n",
            " 38  word_freq_pm                4601 non-null   float64\n",
            " 39  word_freq_direct            4601 non-null   float64\n",
            " 40  word_freq_cs                4601 non-null   float64\n",
            " 41  word_freq_meeting           4601 non-null   float64\n",
            " 42  word_freq_original          4601 non-null   float64\n",
            " 43  word_freq_project           4601 non-null   float64\n",
            " 44  word_freq_re                4601 non-null   float64\n",
            " 45  word_freq_edu               4601 non-null   float64\n",
            " 46  word_freq_table             4601 non-null   float64\n",
            " 47  word_freq_conference        4601 non-null   float64\n",
            " 48  char_freq_;                 4601 non-null   float64\n",
            " 49  char_freq_(                 4601 non-null   float64\n",
            " 50  char_freq_[                 4601 non-null   float64\n",
            " 51  char_freq_!                 4601 non-null   float64\n",
            " 52  char_freq_$                 4601 non-null   float64\n",
            " 53  char_freq_hash              4601 non-null   float64\n",
            " 54  capital_run_length_average  4601 non-null   float64\n",
            " 55  capital_run_length_longest  4601 non-null   int64  \n",
            " 56  capital_run_length_total    4601 non-null   int64  \n",
            " 57  spam                        4601 non-null   int64  \n",
            "dtypes: float64(55), int64(3)\n",
            "memory usage: 2.0 MB\n"
          ]
        }
      ],
      "source": [
        "# Check data type information\n",
        "email_rec.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-1FFLnM0X9A",
        "outputId": "3bad3c3d-f7f8-4d6f-856d-8e4296de2d15"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "word_freq_make                0\n",
              "word_freq_address             0\n",
              "word_freq_all                 0\n",
              "word_freq_3d                  0\n",
              "word_freq_our                 0\n",
              "word_freq_over                0\n",
              "word_freq_remove              0\n",
              "word_freq_internet            0\n",
              "word_freq_order               0\n",
              "word_freq_mail                0\n",
              "word_freq_receive             0\n",
              "word_freq_will                0\n",
              "word_freq_people              0\n",
              "word_freq_report              0\n",
              "word_freq_addresses           0\n",
              "word_freq_free                0\n",
              "word_freq_business            0\n",
              "word_freq_email               0\n",
              "word_freq_you                 0\n",
              "word_freq_credit              0\n",
              "word_freq_your                0\n",
              "word_freq_font                0\n",
              "word_freq_000                 0\n",
              "word_freq_money               0\n",
              "word_freq_hp                  0\n",
              "word_freq_hpl                 0\n",
              "word_freq_george              0\n",
              "word_freq_650                 0\n",
              "word_freq_lab                 0\n",
              "word_freq_labs                0\n",
              "word_freq_telnet              0\n",
              "word_freq_857                 0\n",
              "word_freq_data                0\n",
              "word_freq_415                 0\n",
              "word_freq_85                  0\n",
              "word_freq_technology          0\n",
              "word_freq_1999                0\n",
              "word_freq_parts               0\n",
              "word_freq_pm                  0\n",
              "word_freq_direct              0\n",
              "word_freq_cs                  0\n",
              "word_freq_meeting             0\n",
              "word_freq_original            0\n",
              "word_freq_project             0\n",
              "word_freq_re                  0\n",
              "word_freq_edu                 0\n",
              "word_freq_table               0\n",
              "word_freq_conference          0\n",
              "char_freq_;                   0\n",
              "char_freq_(                   0\n",
              "char_freq_[                   0\n",
              "char_freq_!                   0\n",
              "char_freq_$                   0\n",
              "char_freq_hash                0\n",
              "capital_run_length_average    0\n",
              "capital_run_length_longest    0\n",
              "capital_run_length_total      0\n",
              "spam                          0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Check if any missing values in the dataset \n",
        "email_rec.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJHtfgAL0X9B"
      },
      "source": [
        "Let's also look at the fraction of spam and ham emails in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-48Jvy7u0X9B",
        "outputId": "83c53aad-710e-4384-cff9-ad690b6673fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    60.595523\n",
              "1    39.404477\n",
              "Name: spam, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# look at fraction of spam emails \n",
        "email_rec['spam'].value_counts()*100/email_rec.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcMNj2OT0X9B"
      },
      "source": [
        "You can see that we have 40% of spam mails and 60% of ham"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luR0VW8f0X9B"
      },
      "source": [
        "\n",
        "## Data Preparation\n",
        "\n",
        "Let's now conduct some prelimininary data preparation steps, i.e. rescaling the variables, splitting into train and test etc. To understand why rescaling is required, let's print the summary stats of all columns - you'll notice that the columns at the end (capital_run_length_longest, capital_run_length_total etc.) have much higher values (means = 52, 283 etc.) than most other columns which represent fraction of word occurrences (no. of times word appears in email/total no. of words in email)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "5ew6E9FF0X9B",
        "outputId": "7a5afdce-986d-4cc6-f131-4770ac3a848d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-240236b7-fad1-444e-865e-fb347cc61c47\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_freq_make</th>\n",
              "      <th>word_freq_address</th>\n",
              "      <th>word_freq_all</th>\n",
              "      <th>word_freq_3d</th>\n",
              "      <th>word_freq_our</th>\n",
              "      <th>word_freq_over</th>\n",
              "      <th>word_freq_remove</th>\n",
              "      <th>word_freq_internet</th>\n",
              "      <th>word_freq_order</th>\n",
              "      <th>word_freq_mail</th>\n",
              "      <th>word_freq_receive</th>\n",
              "      <th>word_freq_will</th>\n",
              "      <th>word_freq_people</th>\n",
              "      <th>word_freq_report</th>\n",
              "      <th>word_freq_addresses</th>\n",
              "      <th>word_freq_free</th>\n",
              "      <th>word_freq_business</th>\n",
              "      <th>word_freq_email</th>\n",
              "      <th>word_freq_you</th>\n",
              "      <th>word_freq_credit</th>\n",
              "      <th>word_freq_your</th>\n",
              "      <th>word_freq_font</th>\n",
              "      <th>word_freq_000</th>\n",
              "      <th>word_freq_money</th>\n",
              "      <th>word_freq_hp</th>\n",
              "      <th>word_freq_hpl</th>\n",
              "      <th>word_freq_george</th>\n",
              "      <th>word_freq_650</th>\n",
              "      <th>word_freq_lab</th>\n",
              "      <th>word_freq_labs</th>\n",
              "      <th>word_freq_telnet</th>\n",
              "      <th>word_freq_857</th>\n",
              "      <th>word_freq_data</th>\n",
              "      <th>word_freq_415</th>\n",
              "      <th>word_freq_85</th>\n",
              "      <th>word_freq_technology</th>\n",
              "      <th>word_freq_1999</th>\n",
              "      <th>word_freq_parts</th>\n",
              "      <th>word_freq_pm</th>\n",
              "      <th>word_freq_direct</th>\n",
              "      <th>word_freq_cs</th>\n",
              "      <th>word_freq_meeting</th>\n",
              "      <th>word_freq_original</th>\n",
              "      <th>word_freq_project</th>\n",
              "      <th>word_freq_re</th>\n",
              "      <th>word_freq_edu</th>\n",
              "      <th>word_freq_table</th>\n",
              "      <th>word_freq_conference</th>\n",
              "      <th>char_freq_;</th>\n",
              "      <th>char_freq_(</th>\n",
              "      <th>char_freq_[</th>\n",
              "      <th>char_freq_!</th>\n",
              "      <th>char_freq_$</th>\n",
              "      <th>char_freq_hash</th>\n",
              "      <th>capital_run_length_average</th>\n",
              "      <th>capital_run_length_longest</th>\n",
              "      <th>capital_run_length_total</th>\n",
              "      <th>spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "      <td>4601.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.104553</td>\n",
              "      <td>0.213015</td>\n",
              "      <td>0.280656</td>\n",
              "      <td>0.065425</td>\n",
              "      <td>0.312223</td>\n",
              "      <td>0.095901</td>\n",
              "      <td>0.114208</td>\n",
              "      <td>0.105295</td>\n",
              "      <td>0.090067</td>\n",
              "      <td>0.239413</td>\n",
              "      <td>0.059824</td>\n",
              "      <td>0.541702</td>\n",
              "      <td>0.093930</td>\n",
              "      <td>0.058626</td>\n",
              "      <td>0.049205</td>\n",
              "      <td>0.248848</td>\n",
              "      <td>0.142586</td>\n",
              "      <td>0.184745</td>\n",
              "      <td>1.662100</td>\n",
              "      <td>0.085577</td>\n",
              "      <td>0.809761</td>\n",
              "      <td>0.121202</td>\n",
              "      <td>0.101645</td>\n",
              "      <td>0.094269</td>\n",
              "      <td>0.549504</td>\n",
              "      <td>0.265384</td>\n",
              "      <td>0.767305</td>\n",
              "      <td>0.124845</td>\n",
              "      <td>0.098915</td>\n",
              "      <td>0.102852</td>\n",
              "      <td>0.064753</td>\n",
              "      <td>0.047048</td>\n",
              "      <td>0.097229</td>\n",
              "      <td>0.047835</td>\n",
              "      <td>0.105412</td>\n",
              "      <td>0.097477</td>\n",
              "      <td>0.136953</td>\n",
              "      <td>0.013201</td>\n",
              "      <td>0.078629</td>\n",
              "      <td>0.064834</td>\n",
              "      <td>0.043667</td>\n",
              "      <td>0.132339</td>\n",
              "      <td>0.046099</td>\n",
              "      <td>0.079196</td>\n",
              "      <td>0.301224</td>\n",
              "      <td>0.179824</td>\n",
              "      <td>0.005444</td>\n",
              "      <td>0.031869</td>\n",
              "      <td>0.038575</td>\n",
              "      <td>0.139030</td>\n",
              "      <td>0.016976</td>\n",
              "      <td>0.269071</td>\n",
              "      <td>0.075811</td>\n",
              "      <td>0.044238</td>\n",
              "      <td>5.191515</td>\n",
              "      <td>52.172789</td>\n",
              "      <td>283.289285</td>\n",
              "      <td>0.394045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.305358</td>\n",
              "      <td>1.290575</td>\n",
              "      <td>0.504143</td>\n",
              "      <td>1.395151</td>\n",
              "      <td>0.672513</td>\n",
              "      <td>0.273824</td>\n",
              "      <td>0.391441</td>\n",
              "      <td>0.401071</td>\n",
              "      <td>0.278616</td>\n",
              "      <td>0.644755</td>\n",
              "      <td>0.201545</td>\n",
              "      <td>0.861698</td>\n",
              "      <td>0.301036</td>\n",
              "      <td>0.335184</td>\n",
              "      <td>0.258843</td>\n",
              "      <td>0.825792</td>\n",
              "      <td>0.444055</td>\n",
              "      <td>0.531122</td>\n",
              "      <td>1.775481</td>\n",
              "      <td>0.509767</td>\n",
              "      <td>1.200810</td>\n",
              "      <td>1.025756</td>\n",
              "      <td>0.350286</td>\n",
              "      <td>0.442636</td>\n",
              "      <td>1.671349</td>\n",
              "      <td>0.886955</td>\n",
              "      <td>3.367292</td>\n",
              "      <td>0.538576</td>\n",
              "      <td>0.593327</td>\n",
              "      <td>0.456682</td>\n",
              "      <td>0.403393</td>\n",
              "      <td>0.328559</td>\n",
              "      <td>0.555907</td>\n",
              "      <td>0.329445</td>\n",
              "      <td>0.532260</td>\n",
              "      <td>0.402623</td>\n",
              "      <td>0.423451</td>\n",
              "      <td>0.220651</td>\n",
              "      <td>0.434672</td>\n",
              "      <td>0.349916</td>\n",
              "      <td>0.361205</td>\n",
              "      <td>0.766819</td>\n",
              "      <td>0.223812</td>\n",
              "      <td>0.621976</td>\n",
              "      <td>1.011687</td>\n",
              "      <td>0.911119</td>\n",
              "      <td>0.076274</td>\n",
              "      <td>0.285735</td>\n",
              "      <td>0.243471</td>\n",
              "      <td>0.270355</td>\n",
              "      <td>0.109394</td>\n",
              "      <td>0.815672</td>\n",
              "      <td>0.245882</td>\n",
              "      <td>0.429342</td>\n",
              "      <td>31.729449</td>\n",
              "      <td>194.891310</td>\n",
              "      <td>606.347851</td>\n",
              "      <td>0.488698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.588000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.310000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.220000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.065000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.276000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>95.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.420000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.380000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.160000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.640000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.270000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.110000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.188000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.315000</td>\n",
              "      <td>0.052000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.706000</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>266.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>4.540000</td>\n",
              "      <td>14.280000</td>\n",
              "      <td>5.100000</td>\n",
              "      <td>42.810000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>5.880000</td>\n",
              "      <td>7.270000</td>\n",
              "      <td>11.110000</td>\n",
              "      <td>5.260000</td>\n",
              "      <td>18.180000</td>\n",
              "      <td>2.610000</td>\n",
              "      <td>9.670000</td>\n",
              "      <td>5.550000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>4.410000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>7.140000</td>\n",
              "      <td>9.090000</td>\n",
              "      <td>18.750000</td>\n",
              "      <td>18.180000</td>\n",
              "      <td>11.110000</td>\n",
              "      <td>17.100000</td>\n",
              "      <td>5.450000</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>20.830000</td>\n",
              "      <td>16.660000</td>\n",
              "      <td>33.330000</td>\n",
              "      <td>9.090000</td>\n",
              "      <td>14.280000</td>\n",
              "      <td>5.880000</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>4.760000</td>\n",
              "      <td>18.180000</td>\n",
              "      <td>4.760000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>7.690000</td>\n",
              "      <td>6.890000</td>\n",
              "      <td>8.330000</td>\n",
              "      <td>11.110000</td>\n",
              "      <td>4.760000</td>\n",
              "      <td>7.140000</td>\n",
              "      <td>14.280000</td>\n",
              "      <td>3.570000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>21.420000</td>\n",
              "      <td>22.050000</td>\n",
              "      <td>2.170000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>4.385000</td>\n",
              "      <td>9.752000</td>\n",
              "      <td>4.081000</td>\n",
              "      <td>32.478000</td>\n",
              "      <td>6.003000</td>\n",
              "      <td>19.829000</td>\n",
              "      <td>1102.500000</td>\n",
              "      <td>9989.000000</td>\n",
              "      <td>15841.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-240236b7-fad1-444e-865e-fb347cc61c47')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-240236b7-fad1-444e-865e-fb347cc61c47 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-240236b7-fad1-444e-865e-fb347cc61c47');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       word_freq_make  word_freq_address  ...  capital_run_length_total         spam\n",
              "count     4601.000000        4601.000000  ...               4601.000000  4601.000000\n",
              "mean         0.104553           0.213015  ...                283.289285     0.394045\n",
              "std          0.305358           1.290575  ...                606.347851     0.488698\n",
              "min          0.000000           0.000000  ...                  1.000000     0.000000\n",
              "25%          0.000000           0.000000  ...                 35.000000     0.000000\n",
              "50%          0.000000           0.000000  ...                 95.000000     0.000000\n",
              "75%          0.000000           0.000000  ...                266.000000     1.000000\n",
              "max          4.540000          14.280000  ...              15841.000000     1.000000\n",
              "\n",
              "[8 rows x 58 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# print the summary stats of all columns\n",
        "email_rec.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LOYL3B_R0X9B"
      },
      "outputs": [],
      "source": [
        "# splitting data into X (having all features) and y (having target i.e spam)\n",
        "X = email_rec.drop('spam', axis=1)\n",
        "\n",
        "y = email_rec['spam'].values.astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSlVSPP40X9C"
      },
      "source": [
        "#### Scaling the features\n",
        "\n",
        "We will Standardize features by removing the mean and scaling to unit variance\n",
        "\n",
        "The standard score of a sample x is calculated as:\n",
        "\n",
        "x = x-mean(x)/std(x)\n",
        "\n",
        "Note: Scale() is Equivalent function without the estimator API and has risk of data leak\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "nyHHQKoy0X9C"
      },
      "outputs": [],
      "source": [
        "# Instantiate StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# fit and transform X\n",
        "X = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TX6HcJnS0X9C"
      },
      "outputs": [],
      "source": [
        "# split into train and test with test_size as 30% and random_state as 4\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7lrwZBA0X9C"
      },
      "source": [
        "## Model Building\n",
        "\n",
        "Let's build a linear SVM model now. The ```SVC()``` class does that in sklearn. We highly recommend reading the documentation at least once.\n",
        "\n",
        "Reference doc: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "\n",
        "You can also refer help section for SVC given below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhbGA8en0X9C",
        "outputId": "982569a2-5f0a-489b-c312-d135d2a85788"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class SVC in module sklearn.svm._classes:\n",
            "\n",
            "class SVC(sklearn.svm._base.BaseSVC)\n",
            " |  SVC(*, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)\n",
            " |  \n",
            " |  C-Support Vector Classification.\n",
            " |  \n",
            " |  The implementation is based on libsvm. The fit time scales at least\n",
            " |  quadratically with the number of samples and may be impractical\n",
            " |  beyond tens of thousands of samples. For large datasets\n",
            " |  consider using :class:`~sklearn.svm.LinearSVC` or\n",
            " |  :class:`~sklearn.linear_model.SGDClassifier` instead, possibly after a\n",
            " |  :class:`~sklearn.kernel_approximation.Nystroem` transformer.\n",
            " |  \n",
            " |  The multiclass support is handled according to a one-vs-one scheme.\n",
            " |  \n",
            " |  For details on the precise mathematical formulation of the provided\n",
            " |  kernel functions and how `gamma`, `coef0` and `degree` affect each\n",
            " |  other, see the corresponding section in the narrative documentation:\n",
            " |  :ref:`svm_kernels`.\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <svm_classification>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  C : float, default=1.0\n",
            " |      Regularization parameter. The strength of the regularization is\n",
            " |      inversely proportional to C. Must be strictly positive. The penalty\n",
            " |      is a squared l2 penalty.\n",
            " |  \n",
            " |  kernel : {'linear', 'poly', 'rbf', 'sigmoid', 'precomputed'} or callable,          default='rbf'\n",
            " |      Specifies the kernel type to be used in the algorithm.\n",
            " |      If none is given, 'rbf' will be used. If a callable is given it is\n",
            " |      used to pre-compute the kernel matrix from data matrices; that matrix\n",
            " |      should be an array of shape ``(n_samples, n_samples)``.\n",
            " |  \n",
            " |  degree : int, default=3\n",
            " |      Degree of the polynomial kernel function ('poly').\n",
            " |      Ignored by all other kernels.\n",
            " |  \n",
            " |  gamma : {'scale', 'auto'} or float, default='scale'\n",
            " |      Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.\n",
            " |  \n",
            " |      - if ``gamma='scale'`` (default) is passed then it uses\n",
            " |        1 / (n_features * X.var()) as value of gamma,\n",
            " |      - if 'auto', uses 1 / n_features.\n",
            " |  \n",
            " |      .. versionchanged:: 0.22\n",
            " |         The default value of ``gamma`` changed from 'auto' to 'scale'.\n",
            " |  \n",
            " |  coef0 : float, default=0.0\n",
            " |      Independent term in kernel function.\n",
            " |      It is only significant in 'poly' and 'sigmoid'.\n",
            " |  \n",
            " |  shrinking : bool, default=True\n",
            " |      Whether to use the shrinking heuristic.\n",
            " |      See the :ref:`User Guide <shrinking_svm>`.\n",
            " |  \n",
            " |  probability : bool, default=False\n",
            " |      Whether to enable probability estimates. This must be enabled prior\n",
            " |      to calling `fit`, will slow down that method as it internally uses\n",
            " |      5-fold cross-validation, and `predict_proba` may be inconsistent with\n",
            " |      `predict`. Read more in the :ref:`User Guide <scores_probabilities>`.\n",
            " |  \n",
            " |  tol : float, default=1e-3\n",
            " |      Tolerance for stopping criterion.\n",
            " |  \n",
            " |  cache_size : float, default=200\n",
            " |      Specify the size of the kernel cache (in MB).\n",
            " |  \n",
            " |  class_weight : dict or 'balanced', default=None\n",
            " |      Set the parameter C of class i to class_weight[i]*C for\n",
            " |      SVC. If not given, all classes are supposed to have\n",
            " |      weight one.\n",
            " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
            " |      weights inversely proportional to class frequencies in the input data\n",
            " |      as ``n_samples / (n_classes * np.bincount(y))``.\n",
            " |  \n",
            " |  verbose : bool, default=False\n",
            " |      Enable verbose output. Note that this setting takes advantage of a\n",
            " |      per-process runtime setting in libsvm that, if enabled, may not work\n",
            " |      properly in a multithreaded context.\n",
            " |  \n",
            " |  max_iter : int, default=-1\n",
            " |      Hard limit on iterations within solver, or -1 for no limit.\n",
            " |  \n",
            " |  decision_function_shape : {'ovo', 'ovr'}, default='ovr'\n",
            " |      Whether to return a one-vs-rest ('ovr') decision function of shape\n",
            " |      (n_samples, n_classes) as all other classifiers, or the original\n",
            " |      one-vs-one ('ovo') decision function of libsvm which has shape\n",
            " |      (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one\n",
            " |      ('ovo') is always used as multi-class strategy. The parameter is\n",
            " |      ignored for binary classification.\n",
            " |  \n",
            " |      .. versionchanged:: 0.19\n",
            " |          decision_function_shape is 'ovr' by default.\n",
            " |  \n",
            " |      .. versionadded:: 0.17\n",
            " |         *decision_function_shape='ovr'* is recommended.\n",
            " |  \n",
            " |      .. versionchanged:: 0.17\n",
            " |         Deprecated *decision_function_shape='ovo' and None*.\n",
            " |  \n",
            " |  break_ties : bool, default=False\n",
            " |      If true, ``decision_function_shape='ovr'``, and number of classes > 2,\n",
            " |      :term:`predict` will break ties according to the confidence values of\n",
            " |      :term:`decision_function`; otherwise the first class among the tied\n",
            " |      classes is returned. Please note that breaking ties comes at a\n",
            " |      relatively high computational cost compared to a simple predict.\n",
            " |  \n",
            " |      .. versionadded:: 0.22\n",
            " |  \n",
            " |  random_state : int, RandomState instance or None, default=None\n",
            " |      Controls the pseudo random number generation for shuffling the data for\n",
            " |      probability estimates. Ignored when `probability` is False.\n",
            " |      Pass an int for reproducible output across multiple function calls.\n",
            " |      See :term:`Glossary <random_state>`.\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  class_weight_ : ndarray of shape (n_classes,)\n",
            " |      Multipliers of parameter C for each class.\n",
            " |      Computed based on the ``class_weight`` parameter.\n",
            " |  \n",
            " |  classes_ : ndarray of shape (n_classes,)\n",
            " |      The classes labels.\n",
            " |  \n",
            " |  coef_ : ndarray of shape (n_classes * (n_classes - 1) / 2, n_features)\n",
            " |      Weights assigned to the features (coefficients in the primal\n",
            " |      problem). This is only available in the case of a linear kernel.\n",
            " |  \n",
            " |      `coef_` is a readonly property derived from `dual_coef_` and\n",
            " |      `support_vectors_`.\n",
            " |  \n",
            " |  dual_coef_ : ndarray of shape (n_classes -1, n_SV)\n",
            " |      Dual coefficients of the support vector in the decision\n",
            " |      function (see :ref:`sgd_mathematical_formulation`), multiplied by\n",
            " |      their targets.\n",
            " |      For multiclass, coefficient for all 1-vs-1 classifiers.\n",
            " |      The layout of the coefficients in the multiclass case is somewhat\n",
            " |      non-trivial. See the :ref:`multi-class section of the User Guide\n",
            " |      <svm_multi_class>` for details.\n",
            " |  \n",
            " |  fit_status_ : int\n",
            " |      0 if correctly fitted, 1 otherwise (will raise warning)\n",
            " |  \n",
            " |  intercept_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)\n",
            " |      Constants in decision function.\n",
            " |  \n",
            " |  n_features_in_ : int\n",
            " |      Number of features seen during :term:`fit`.\n",
            " |  \n",
            " |      .. versionadded:: 0.24\n",
            " |  \n",
            " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
            " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
            " |      has feature names that are all strings.\n",
            " |  \n",
            " |      .. versionadded:: 1.0\n",
            " |  \n",
            " |  support_ : ndarray of shape (n_SV)\n",
            " |      Indices of support vectors.\n",
            " |  \n",
            " |  support_vectors_ : ndarray of shape (n_SV, n_features)\n",
            " |      Support vectors.\n",
            " |  \n",
            " |  n_support_ : ndarray of shape (n_classes,), dtype=int32\n",
            " |      Number of support vectors for each class.\n",
            " |  \n",
            " |  probA_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\n",
            " |  probB_ : ndarray of shape (n_classes * (n_classes - 1) / 2)\n",
            " |      If `probability=True`, it corresponds to the parameters learned in\n",
            " |      Platt scaling to produce probability estimates from decision values.\n",
            " |      If `probability=False`, it's an empty array. Platt scaling uses the\n",
            " |      logistic function\n",
            " |      ``1 / (1 + exp(decision_value * probA_ + probB_))``\n",
            " |      where ``probA_`` and ``probB_`` are learned from the dataset [2]_. For\n",
            " |      more information on the multiclass case and training procedure see\n",
            " |      section 8 of [1]_.\n",
            " |  \n",
            " |  shape_fit_ : tuple of int of shape (n_dimensions_of_X,)\n",
            " |      Array dimensions of training vector ``X``.\n",
            " |  \n",
            " |  See Also\n",
            " |  --------\n",
            " |  SVR : Support Vector Machine for Regression implemented using libsvm.\n",
            " |  \n",
            " |  LinearSVC : Scalable Linear Support Vector Machine for classification\n",
            " |      implemented using liblinear. Check the See Also section of\n",
            " |      LinearSVC for more comparison element.\n",
            " |  \n",
            " |  References\n",
            " |  ----------\n",
            " |  .. [1] `LIBSVM: A Library for Support Vector Machines\n",
            " |      <http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_\n",
            " |  \n",
            " |  .. [2] `Platt, John (1999). \"Probabilistic outputs for support vector\n",
            " |      machines and comparison to regularizedlikelihood methods.\"\n",
            " |      <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.1639>`_\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> import numpy as np\n",
            " |  >>> from sklearn.pipeline import make_pipeline\n",
            " |  >>> from sklearn.preprocessing import StandardScaler\n",
            " |  >>> X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])\n",
            " |  >>> y = np.array([1, 1, 2, 2])\n",
            " |  >>> from sklearn.svm import SVC\n",
            " |  >>> clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
            " |  >>> clf.fit(X, y)\n",
            " |  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
            " |                  ('svc', SVC(gamma='auto'))])\n",
            " |  \n",
            " |  >>> print(clf.predict([[-0.8, -1]]))\n",
            " |  [1]\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      SVC\n",
            " |      sklearn.svm._base.BaseSVC\n",
            " |      sklearn.base.ClassifierMixin\n",
            " |      sklearn.svm._base.BaseLibSVM\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, *, C=1.0, kernel='rbf', degree=3, gamma='scale', coef0=0.0, shrinking=True, probability=False, tol=0.001, cache_size=200, class_weight=None, verbose=False, max_iter=-1, decision_function_shape='ovr', break_ties=False, random_state=None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.svm._base.BaseSVC:\n",
            " |  \n",
            " |  decision_function(self, X)\n",
            " |      Evaluate the decision function for the samples in X.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          The input samples.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X : ndarray of shape (n_samples, n_classes * (n_classes-1) / 2)\n",
            " |          Returns the decision function of the sample for each class\n",
            " |          in the model.\n",
            " |          If decision_function_shape='ovr', the shape is (n_samples,\n",
            " |          n_classes).\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      If decision_function_shape='ovo', the function values are proportional\n",
            " |      to the distance of the samples X to the separating hyperplane. If the\n",
            " |      exact distances are required, divide the function values by the norm of\n",
            " |      the weight vector (``coef_``). See also `this question\n",
            " |      <https://stats.stackexchange.com/questions/14876/\n",
            " |      interpreting-distance-from-hyperplane-in-svm>`_ for further details.\n",
            " |      If decision_function_shape='ovr', the decision function is a monotonic\n",
            " |      transformation of ovo decision function.\n",
            " |  \n",
            " |  predict(self, X)\n",
            " |      Perform classification on samples in X.\n",
            " |      \n",
            " |      For an one-class model, +1 or -1 is returned.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features) or                 (n_samples_test, n_samples_train)\n",
            " |          For kernel=\"precomputed\", the expected shape of X is\n",
            " |          (n_samples_test, n_samples_train).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      y_pred : ndarray of shape (n_samples,)\n",
            " |          Class labels for samples in X.\n",
            " |  \n",
            " |  predict_log_proba(self, X)\n",
            " |      Compute log probabilities of possible outcomes for samples in X.\n",
            " |      \n",
            " |      The model need to have probability information computed at training\n",
            " |      time: fit with attribute `probability` set to True.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features) or                 (n_samples_test, n_samples_train)\n",
            " |          For kernel=\"precomputed\", the expected shape of X is\n",
            " |          (n_samples_test, n_samples_train).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      T : ndarray of shape (n_samples, n_classes)\n",
            " |          Returns the log-probabilities of the sample for each class in\n",
            " |          the model. The columns correspond to the classes in sorted\n",
            " |          order, as they appear in the attribute :term:`classes_`.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      The probability model is created using cross validation, so\n",
            " |      the results can be slightly different than those obtained by\n",
            " |      predict. Also, it will produce meaningless results on very small\n",
            " |      datasets.\n",
            " |  \n",
            " |  predict_proba(self, X)\n",
            " |      Compute probabilities of possible outcomes for samples in X.\n",
            " |      \n",
            " |      The model need to have probability information computed at training\n",
            " |      time: fit with attribute `probability` set to True.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          For kernel=\"precomputed\", the expected shape of X is\n",
            " |          (n_samples_test, n_samples_train).\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      T : ndarray of shape (n_samples, n_classes)\n",
            " |          Returns the probability of the sample for each class in\n",
            " |          the model. The columns correspond to the classes in sorted\n",
            " |          order, as they appear in the attribute :term:`classes_`.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      The probability model is created using cross validation, so\n",
            " |      the results can be slightly different than those obtained by\n",
            " |      predict. Also, it will produce meaningless results on very small\n",
            " |      datasets.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.svm._base.BaseSVC:\n",
            " |  \n",
            " |  probA_\n",
            " |      Parameter learned in Platt scaling when `probability=True`.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      ndarray of shape  (n_classes * (n_classes - 1) / 2)\n",
            " |  \n",
            " |  probB_\n",
            " |      Parameter learned in Platt scaling when `probability=True`.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      ndarray of shape  (n_classes * (n_classes - 1) / 2)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Return the mean accuracy on the given test data and labels.\n",
            " |      \n",
            " |      In multi-label classification, this is the subset accuracy\n",
            " |      which is a harsh metric since you require for each sample that\n",
            " |      each label set be correctly predicted.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Test samples.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          True labels for `X`.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.svm._base.BaseLibSVM:\n",
            " |  \n",
            " |  fit(self, X, y, sample_weight=None)\n",
            " |      Fit the SVM model according to the given training data.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)                 or (n_samples, n_samples)\n",
            " |          Training vectors, where `n_samples` is the number of samples\n",
            " |          and `n_features` is the number of features.\n",
            " |          For kernel=\"precomputed\", the expected shape of X is\n",
            " |          (n_samples, n_samples).\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,)\n",
            " |          Target values (class labels in classification, real numbers in\n",
            " |          regression).\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Per-sample weights. Rescale C per sample. Higher weights\n",
            " |          force the classifier to put more emphasis on these points.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Fitted estimator.\n",
            " |      \n",
            " |      Notes\n",
            " |      -----\n",
            " |      If X and y are not C-ordered and contiguous arrays of np.float64 and\n",
            " |      X is not a scipy.sparse.csr_matrix, X and/or y may be copied.\n",
            " |      \n",
            " |      If X is a dense array, then the other methods will not support sparse\n",
            " |      matrices as input.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.svm._base.BaseLibSVM:\n",
            " |  \n",
            " |  coef_\n",
            " |      Weights assigned to the features when `kernel=\"linear\"`.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      ndarray of shape (n_features, n_classes)\n",
            " |  \n",
            " |  n_support_\n",
            " |      Number of support vectors for each class.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : dict\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
            " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
            " |      possible to update each component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : estimator instance\n",
            " |          Estimator instance.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "help(SVC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzTqH0QQ0X9C"
      },
      "source": [
        "### Model building\n",
        "\n",
        "Firstly you are going to use the default kernal i.e RBF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "cdR2WK8U0X9D"
      },
      "outputs": [],
      "source": [
        "# Instantiate an object of class SVC(), using cost C=1\n",
        "model = SVC(C=1)\n",
        "\n",
        "# fit model on training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# predict on test data\n",
        "y_pred_rbf = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkxpXuzm0X9D"
      },
      "source": [
        "##### Evaluate the model using confusion matrix \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-BURBrjP0X9D",
        "outputId": "e8dfe4f2-798d-4bd4-b6fc-cf4c719420ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[811,  38],\n",
              "       [ 61, 471]])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# import metrics\n",
        "from sklearn import metrics\n",
        "\n",
        "# print confusion_matrix\n",
        "metrics.confusion_matrix(y_true=y_test, y_pred=y_pred_rbf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KfisNGQ0X9D"
      },
      "source": [
        "#### Print other metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "nSMuTKXN0X9D",
        "outputId": "b03f3f32-2c3d-47e3-efe7-66ae71a61708",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.9283128167994207\n",
            "precision 0.925343811394892\n",
            "recall 0.8853383458646616\n"
          ]
        }
      ],
      "source": [
        "# accuracy\n",
        "print(\"accuracy\", metrics.accuracy_score(y_test, y_pred_rbf))\n",
        "\n",
        "# precision\n",
        "print(\"precision\", metrics.precision_score(y_test, y_pred_rbf))\n",
        "\n",
        "# recall/sensitivity\n",
        "print(\"recall\", metrics.recall_score(y_test, y_pred_rbf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "KgJttyUZ0X9D",
        "outputId": "ccc94bcf-fc65-4f9e-9555-cc9fd77bd8ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "specificity 0.9552414605418139\n"
          ]
        }
      ],
      "source": [
        "# specificity (% of hams correctly classified)\n",
        "print(\"specificity\", 811/(811+38))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4TF8h-_0X9D"
      },
      "source": [
        "The SVM we have built so far gives decently good results - an accuracy of 92%, sensitivity/recall (TNR) of 88%. \n",
        "\n",
        "### Interpretation of Results\n",
        "\n",
        "In the confusion matrix, the elements at (0, 0) and (1,1) correspond to the more frequently occurring class, i.e. ham emails. Thus, it implies that:\n",
        "- 92% of all emails are classified correctly\n",
        "- 88.5% of spams are identified correctly (sensitivity/recall)\n",
        "- Specificity, or % of hams classified correctly, is 95%\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xU-O00390X9E"
      },
      "source": [
        "### K-Fold Cross Validation\n",
        "\n",
        "Let's first run a simple k-fold cross validation to get a sense of the **average metrics** as computed over multiple *folds*. the easiest way to do cross-validation is to use the ```cross_val_score()``` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ltINkXTs0X9E"
      },
      "outputs": [],
      "source": [
        "# creating a KFold object with 5 splits , shuffle True and random_state=4\n",
        "folds = KFold(n_splits=5, shuffle=True, random_state=4)\n",
        "\n",
        "# instantiating a SVC model with cost=1 and random_state=4\n",
        "model = SVC(C=1, random_state=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z97OnyMX0X9E"
      },
      "source": [
        "Computing the cross-validation scores 1(note that the argument cv takes the 'folds' object, and we have specified 'accuracy' as the metric\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "7xFYLs7w0X9E"
      },
      "outputs": [],
      "source": [
        "# Computing the cross-validation scores\n",
        "cv_results = cross_val_score(model, X_train, y_train, cv=folds, scoring='accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "-Y0QHsd10X9E",
        "outputId": "17978a66-820e-445d-f1e6-121811dec814",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.91770186 0.94099379 0.92080745 0.93012422 0.93944099]\n",
            "mean accuracy = 0.9298136645962731\n"
          ]
        }
      ],
      "source": [
        "# print 5 accuracies obtained from the 5 folds\n",
        "print(cv_results)\n",
        "\n",
        "# print mean accuracy of all 5 folds\n",
        "print(\"mean accuracy = {}\".format(cv_results.mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taDD50g-0X9E"
      },
      "source": [
        "K-fold CV helps us compute average metrics over multiple folds, and that is the best indication of the 'test accuracy/other metric scores' we can have. \n",
        "\n",
        "## Hyperparameter Tuning\n",
        "### Grid Search to Find Optimal Hyperparameter C\n",
        "\n",
        "Now we want to use CV to compute the optimal values of hyperparameters (in this case, the cost C is a hyperparameter). This is done using the ```GridSearchCV()``` method, which computes metrics (such as accuracy, recall etc.) \n",
        "\n",
        "In this case, we have only one hyperparameter, though you can have multiple, such as C and gamma in non-linear SVMs. In that case, you need to search through a *grid* of multiple values of C and gamma to find the optimal combination, and hence the name GridSearchCV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "a7WpdlO-0X9E"
      },
      "outputs": [],
      "source": [
        "# specify range of parameters (C) as a list ([0.1, 1, 10, 100, 1000])\n",
        "params = {\"C\": [0.1, 1, 10, 100, 1000]}\n",
        "\n",
        "# Intialise SVC()\n",
        "model = SVC()\n",
        "\n",
        "\n",
        "'''set up grid search scheme(note that we are still using the 5 fold CV scheme we set up earlier, \n",
        "scoring as accuracy, verbose as 1 and return_train_score as True)'''\n",
        "\n",
        "model_cv = GridSearchCV(estimator=model, param_grid=params,\n",
        "                        scoring='accuracy',\n",
        "                        cv=folds,\n",
        "                        verbose=1,\n",
        "                        return_train_score=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "VgETmREY0X9F",
        "outputId": "c9b6aeae-910b-4d38-e59e-eb743a74fbd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=KFold(n_splits=5, random_state=4, shuffle=True),\n",
              "             estimator=SVC(), param_grid={'C': [0.1, 1, 10, 100, 1000]},\n",
              "             return_train_score=True, scoring='accuracy', verbose=1)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# fit the model on train data, it will fit 5 folds across all values of C\n",
        "model_cv.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "vYhM3RcY0X9F",
        "outputId": "84a9550e-a6a0-4908-b1f5-d1a9387b80c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b91deaa7-ac48-4e8c-910e-f7e4cc5701d3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_C</th>\n",
              "      <th>params</th>\n",
              "      <th>split0_test_score</th>\n",
              "      <th>split1_test_score</th>\n",
              "      <th>split2_test_score</th>\n",
              "      <th>split3_test_score</th>\n",
              "      <th>split4_test_score</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "      <th>split0_train_score</th>\n",
              "      <th>split1_train_score</th>\n",
              "      <th>split2_train_score</th>\n",
              "      <th>split3_train_score</th>\n",
              "      <th>split4_train_score</th>\n",
              "      <th>mean_train_score</th>\n",
              "      <th>std_train_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.307280</td>\n",
              "      <td>0.008584</td>\n",
              "      <td>0.103147</td>\n",
              "      <td>0.001580</td>\n",
              "      <td>0.1</td>\n",
              "      <td>{'C': 0.1}</td>\n",
              "      <td>0.895963</td>\n",
              "      <td>0.902174</td>\n",
              "      <td>0.906832</td>\n",
              "      <td>0.902174</td>\n",
              "      <td>0.919255</td>\n",
              "      <td>0.905280</td>\n",
              "      <td>0.007795</td>\n",
              "      <td>5</td>\n",
              "      <td>0.912655</td>\n",
              "      <td>0.911879</td>\n",
              "      <td>0.912267</td>\n",
              "      <td>0.911102</td>\n",
              "      <td>0.906056</td>\n",
              "      <td>0.910792</td>\n",
              "      <td>0.002423</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.205944</td>\n",
              "      <td>0.005011</td>\n",
              "      <td>0.062545</td>\n",
              "      <td>0.004329</td>\n",
              "      <td>1</td>\n",
              "      <td>{'C': 1}</td>\n",
              "      <td>0.917702</td>\n",
              "      <td>0.940994</td>\n",
              "      <td>0.920807</td>\n",
              "      <td>0.930124</td>\n",
              "      <td>0.939441</td>\n",
              "      <td>0.929814</td>\n",
              "      <td>0.009440</td>\n",
              "      <td>2</td>\n",
              "      <td>0.951475</td>\n",
              "      <td>0.946040</td>\n",
              "      <td>0.949922</td>\n",
              "      <td>0.946040</td>\n",
              "      <td>0.945652</td>\n",
              "      <td>0.947826</td>\n",
              "      <td>0.002401</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.182248</td>\n",
              "      <td>0.010526</td>\n",
              "      <td>0.049391</td>\n",
              "      <td>0.002686</td>\n",
              "      <td>10</td>\n",
              "      <td>{'C': 10}</td>\n",
              "      <td>0.908385</td>\n",
              "      <td>0.944099</td>\n",
              "      <td>0.933230</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.939441</td>\n",
              "      <td>0.930745</td>\n",
              "      <td>0.012368</td>\n",
              "      <td>1</td>\n",
              "      <td>0.975543</td>\n",
              "      <td>0.970885</td>\n",
              "      <td>0.973991</td>\n",
              "      <td>0.968944</td>\n",
              "      <td>0.971273</td>\n",
              "      <td>0.972127</td>\n",
              "      <td>0.002347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.214149</td>\n",
              "      <td>0.014441</td>\n",
              "      <td>0.045279</td>\n",
              "      <td>0.003533</td>\n",
              "      <td>100</td>\n",
              "      <td>{'C': 100}</td>\n",
              "      <td>0.913043</td>\n",
              "      <td>0.927019</td>\n",
              "      <td>0.936335</td>\n",
              "      <td>0.930124</td>\n",
              "      <td>0.936335</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.008562</td>\n",
              "      <td>3</td>\n",
              "      <td>0.989519</td>\n",
              "      <td>0.989907</td>\n",
              "      <td>0.989907</td>\n",
              "      <td>0.988354</td>\n",
              "      <td>0.987966</td>\n",
              "      <td>0.989130</td>\n",
              "      <td>0.000814</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.283800</td>\n",
              "      <td>0.037075</td>\n",
              "      <td>0.040235</td>\n",
              "      <td>0.003095</td>\n",
              "      <td>1000</td>\n",
              "      <td>{'C': 1000}</td>\n",
              "      <td>0.908385</td>\n",
              "      <td>0.931677</td>\n",
              "      <td>0.922360</td>\n",
              "      <td>0.919255</td>\n",
              "      <td>0.920807</td>\n",
              "      <td>0.920497</td>\n",
              "      <td>0.007440</td>\n",
              "      <td>4</td>\n",
              "      <td>0.996118</td>\n",
              "      <td>0.993789</td>\n",
              "      <td>0.994177</td>\n",
              "      <td>0.993789</td>\n",
              "      <td>0.994177</td>\n",
              "      <td>0.994410</td>\n",
              "      <td>0.000872</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b91deaa7-ac48-4e8c-910e-f7e4cc5701d3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b91deaa7-ac48-4e8c-910e-f7e4cc5701d3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b91deaa7-ac48-4e8c-910e-f7e4cc5701d3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   mean_fit_time  std_fit_time  ...  mean_train_score  std_train_score\n",
              "0       0.307280      0.008584  ...          0.910792         0.002423\n",
              "1       0.205944      0.005011  ...          0.947826         0.002401\n",
              "2       0.182248      0.010526  ...          0.972127         0.002347\n",
              "3       0.214149      0.014441  ...          0.989130         0.000814\n",
              "4       0.283800      0.037075  ...          0.994410         0.000872\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# convert results of grid search CV into dataframe\n",
        "cv_results = pd.DataFrame(model_cv.cv_results_)\n",
        "\n",
        "# print cv_results\n",
        "cv_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kc9wq3P0X9F"
      },
      "source": [
        "You can expect a different result in the dataframe at your end as grid search will shuffle the data during cross validation and computes the result.\n",
        "\n",
        "To get a better sense of how training and test accuracy varies with C, let's plot the tranining and test accuracies against C."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "3k7aSUSJ0X9F",
        "outputId": "fc00c932-64a3-4963-f7f6-3f4461e47475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# plot of C versus train and test scores\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(cv_results['param_C'], cv_results['mean_test_score'])\n",
        "plt.plot(cv_results['param_C'], cv_results['mean_train_score'])\n",
        "plt.xlabel('C')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend(['test accuracy', 'train accuracy'], loc='upper left')\n",
        "plt.xscale('log')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XitdYPln0X9F"
      },
      "source": [
        "Though the training accuracy monotonically increases with C, the test accuracy gradually reduces. Thus, we can conclude that higher values of C tend to **overfit** the model. This is because a high C value aims to classify all training examples correctly (since C is the *cost of misclassification* - if you impose a high cost on the model, it will avoid misclassifying any points by overfitting the data). \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01OoMR-I0X9G"
      },
      "source": [
        "Let's finally look at the optimal C values found by GridSearchCV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "O6Dvf-2z0X9G",
        "outputId": "c4be7681-1c45-4708-ae00-c801e3618b29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The highest test accuracy is 0.9307453416149067 at C = 10\n"
          ]
        }
      ],
      "source": [
        "# get best score for above gridearch\n",
        "best_score = model_cv.best_score_\n",
        "\n",
        "# get best c value as well\n",
        "best_C = model_cv.best_params_['C']\n",
        "\n",
        "print(\" The highest test accuracy is {0} at C = {1}\".format(best_score, best_C))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3Y9_4IC0X9G"
      },
      "source": [
        "Let's now look at the metrics corresponding to C=10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "A3MBLeO40X9G"
      },
      "outputs": [],
      "source": [
        "# model with the best value of C\n",
        "model = SVC(C=best_C)\n",
        "\n",
        "# fit model on training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# predict on test data\n",
        "y_pred_rbf1 = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "B7ASE6pj0X9G",
        "outputId": "7788353c-1ab1-4a00-9da3-ad6f50157348",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[810,  39],\n",
              "       [ 57, 475]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "# print confusion_matrix\n",
        "metrics.confusion_matrix(y_true=y_test, y_pred=y_pred_rbf1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKgLyTfs0X9G"
      },
      "source": [
        "You can see type 2 error is reduced after hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![1.JPG](data:image/jpeg;base64,/9j/4AAQSkZJRgABAQEAeAB4AAD/4REiRXhpZgAATU0AKgAAAAgABAE7AAIAAAAdAAAISodpAAQAAAABAAAIaJydAAEAAAA6AAAQ4OocAAcAAAgMAAAAPgAAAAAc6gAAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEFudWdhbnRpIFN1cmVzaCAoQ29uc3VsdGFudCkAAAAFkAMAAgAAABQAABC2kAQAAgAAABQAABDKkpEAAgAAAAMwMgAAkpIAAgAAAAMwMgAA6hwABwAACAwAAAiqAAAAABzqAAAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMjAyMjowMToxNSAxNzozMzo0MAAyMDIyOjAxOjE1IDE3OjMzOjQwAAAAQQBuAHUAZwBhAG4AdABpACAAUwB1AHIAZQBzAGgAIAAoAEMAbwBuAHMAdQBsAHQAYQBuAHQAKQAAAP/hCy9odHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvADw/eHBhY2tldCBiZWdpbj0n77u/JyBpZD0nVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkJz8+DQo8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIj48cmRmOlJERiB4bWxuczpyZGY9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkvMDIvMjItcmRmLXN5bnRheC1ucyMiPjxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSJ1dWlkOmZhZjViZGQ1LWJhM2QtMTFkYS1hZDMxLWQzM2Q3NTE4MmYxYiIgeG1sbnM6ZGM9Imh0dHA6Ly9wdXJsLm9yZy9kYy9lbGVtZW50cy8xLjEvIi8+PHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9InV1aWQ6ZmFmNWJkZDUtYmEzZC0xMWRhLWFkMzEtZDMzZDc1MTgyZjFiIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iPjx4bXA6Q3JlYXRlRGF0ZT4yMDIyLTAxLTE1VDE3OjMzOjQwLjAyNDwveG1wOkNyZWF0ZURhdGU+PC9yZGY6RGVzY3JpcHRpb24+PHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9InV1aWQ6ZmFmNWJkZDUtYmEzZC0xMWRhLWFkMzEtZDMzZDc1MTgyZjFiIiB4bWxuczpkYz0iaHR0cDovL3B1cmwub3JnL2RjL2VsZW1lbnRzLzEuMS8iPjxkYzpjcmVhdG9yPjxyZGY6U2VxIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+PHJkZjpsaT5BbnVnYW50aSBTdXJlc2ggKENvbnN1bHRhbnQpPC9yZGY6bGk+PC9yZGY6U2VxPg0KCQkJPC9kYzpjcmVhdG9yPjwvcmRmOkRlc2NyaXB0aW9uPjwvcmRmOlJERj48L3g6eG1wbWV0YT4NCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgPD94cGFja2V0IGVuZD0ndyc/Pv/bAEMABwUFBgUEBwYFBggHBwgKEQsKCQkKFQ8QDBEYFRoZGBUYFxseJyEbHSUdFxgiLiIlKCkrLCsaIC8zLyoyJyorKv/bAEMBBwgICgkKFAsLFCocGBwqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKv/AABEIAUQCZwMBIgACEQEDEQH/xAAfAAABBQEBAQEBAQAAAAAAAAAAAQIDBAUGBwgJCgv/xAC1EAACAQMDAgQDBQUEBAAAAX0BAgMABBEFEiExQQYTUWEHInEUMoGRoQgjQrHBFVLR8CQzYnKCCQoWFxgZGiUmJygpKjQ1Njc4OTpDREVGR0hJSlNUVVZXWFlaY2RlZmdoaWpzdHV2d3h5eoOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4eLj5OXm5+jp6vHy8/T19vf4+fr/xAAfAQADAQEBAQEBAQEBAAAAAAAAAQIDBAUGBwgJCgv/xAC1EQACAQIEBAMEBwUEBAABAncAAQIDEQQFITEGEkFRB2FxEyIygQgUQpGhscEJIzNS8BVictEKFiQ04SXxFxgZGiYnKCkqNTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqCg4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2dri4+Tl5ufo6ery8/T19vf4+fr/2gAMAwEAAhEDEQA/APo+iiigAooooAKKKKACiiigAooooAKKKKACkxS0UAFJS0UAJijFLSZFABioZbdJByOamyKTNAmkzLltGj5HIquRjrW4cHrVaa0Rx8vBqrnPOj1RmUVJJC0Z5qPNMwaaYtFJkUZ96BC0UZFGRTAQ0UEijIpgFFGRRkUAFFGRRkUwCijIoyKAEooyKMimAUUZFGRQISijIpMimAtFJkUZFAAKDQCKCRQIKKTI9aMimAtJRkUmRQAtFJkUZFMBaKTIoyKAEooyKMigAooyKMimAUlLkUhIpAFFGR60mfemAUUZzU8Nq8x44FDaQ0m3oQgEnA61agsHkOW4FX4LKOIDOCfWrIwOlZSqHTCj1ZHDbJEuAOamxRmjIrJu50JJBijFGRRkUhhilpMijIoAWikyKMigBaKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAOX8a3M8NnBHbSNGZWKkqcGs628JX01tHIdSucsuf8AWGr3jb7tl/10P8q6LT/+QfD/ALgoA5T/AIQ29/6CVz/38NH/AAh17/0Ern/v4a7SkxQBxf8Awh17/wBBK5/7+Gj/AIQ69/6CVz/38NdpijFAHFN4LvHGG1G4/wC/hqnN4BvTympXP/fw16DijFBMoqR5i/gq/Q4bUbn/AL+Gm/8ACHXv/QSuf+/hr0ySFZByKozWbLynIqrnNKk1scD/AMIde/8AQSuf+/ho/wCEOvf+glc/9/DXaMCDgikqjHY4z/hDr3/oJXP/AH8NH/CHXv8A0Ern/v4a7OimBxn/AAh17/0Ern/v4aP+EOvf+glc/wDfw12dFAHGf8Ide/8AQSuf+/ho/wCEOvv+glc/9/DXZ0UwOL/4Q69/6CVz/wB/DR/wht7/ANBK5/7+Gu0ooA4v/hDr3/oJXP8A38NJ/wAIde/9BK5/7+Gu0opgcZ/wh17/ANBK5/7+Gk/4Q69/6CVz/wB/DXaUUCOL/wCEOvf+glc/9/DR/wAIde/9BK5/7+GuzooA4z/hDr3/AKCVz/38NJ/wh17/ANBK5/7+Gu0opgcX/wAIde/9BK5/7+Gg+Dr3/oJXP/fw12YoNAjx/wAKaTqWp67rVvJqdyVtbjYv70+ldZ/wh17/ANBK5/7+GqPw/wD+Rq8Uf9fo/lXoVMDi/wDhDr3/AKCVz/38NJ/wh17/ANBK5/7+Gu1pKAOL/wCEOvf+glc/9/DR/wAIde/9BK5/7+Gu0opgcX/wht7/ANBK5/7+Gj/hDb3/AKCVz/38NdpRQBxX/CHXv/QSuf8Av4aP+EOvf+glc/8Afw12dFAjjP8AhDb3/oJXP/fw0n/CG3v/AEErn/v4a7SimM4v/hDb3/oJXP8A38NH/CHXv/QSuf8Av4a7SlVS5woyaQWucV/wh17/ANBK5/7+Gnx+CNQlbC6jcn/toa7+308tzJxWjHCka4UVEp9jeFFvVnAW/wAPbxeZNSuSf+uhq8vgu8UYXUrgf9tDXa4oxWLbZ1RionF/8Ide/wDQSuf+/ho/4Q69/wCglc/9/DXaYopFHGf8Ide/9BK5/wC/ho/4Q29/6CVz/wB/DXaUUAcX/wAIbe/9BK5/7+Gj/hDb3/oJXP8A38NdpRQBxf8Awht7/wBBK5/7+Gj/AIQ29/6CVz/38NdpRQBxf/CG3v8A0Ern/v4ahuvCV9BaySrqVxlVyP3hruqq6j/yDZ/9ygDD8FXE82nzJcyF2jfbknNdNXLeCP8Aj3vP+u1dTQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQBynjb7tl/10P8AKui0/wD5B8P+4K53xt92y/66H+VdFp//ACD4f9wUAWaKKKACiiigAooooASjFLSUAQTWqS9sGqE1q0fuKvX0jRadcyJ95ImYfUA15ZpWt3s2m22pC7aW5ldg0BbIGD6U09bGc6aaud90orDl8XWx1GOwEEv2jy1eQhflGapy+NrOG7kRslI3CMQO5qk7nNKDidRRXn/i/wAZodI1W1sRMssEIbzVHTPoaj03U4xd2k11eXPmC0V2Td8pGOv1p31ItoeiUVy48XpcRytaQyMqoSH28Hiszwz45a58Lyajqi7CruBgdcGmI7uiuWtPHFjcJJ94ssZkGB2rm7vxVLq3i3QXtTPDBPG5KEYDc96YHplFIn3B9KWmAUUUUCEooopgFFFFACCg0Cg0CPPfh/8A8jV4o/6/R/KvQq89+H//ACNXij/r9H8q9CpgFJS0lABRRRTAKSlpKAEqteaha2CBryZYlPdjVmuD+IdwIL/T2lVniwdyKM559KTdhpXOzs9Qtb+PzLOZZV9VNWQMnivMtPTU9POq67YxtHZC0HlQuMAODycVsp4k1bSPCNvqeoxxtLdSJs284BolKxcKcpHfQWTycngVObmwsJfKmmRJMZIJriD411Kz0nUriSAKlvt8guOufWqj3V5N4kinu/KdJIEkCg5681hKTudcKcUj0u0vrW+VjaTLKEOG2noanaRUZVY4LdB6156fEVzBFJBp0EMUrygZxgYqHxTJqqaxoim42bnOdjdfrUs0R6XSMQqkngAZNebat42v7bXJNMt4t5ithIpAyS3vUs/izVLq50/Twkccl1aPLLu4wQO1HQDvba+trssLaVZCvXB6VYrhvh08skNwbgqXzztPHWu5o6IOrCiiigAooooAKKKKACquo/8AINn/ANyrVVdR/wCQbP8A7lAGB4I/497z/rtXU1y3gj/j3vP+u1dTQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQBynjb7tl/10P8q6LT/+QfD/ALgrnfG33bL/AK6H+VdFp/8AyD4f9wUAWaKKKACiiigAooooAKSlooAgu4jPZTxDgyRso/EYrlvCfgm00exj+1QiS6RifMNdhRR1uPpY5aDw0661dXDnCTRbBx0rEtPA4tr2eOe1Fwksm4Sk4216JSULQlpPc8w1rwDfy/2nFZTEQ30QjHA/d+9QnwTcrPGJJDtFqIGOPbrXpMmp26Xq2u8GVjjbnpTGvbG4keJJkeRRyoPIp3TMpUuxwei6Nf6ZG+nMN1qEZFk9eKyrbwPcp4bk0mRiVjdpEf8AvZPSvQYJobySZbVw7RHDgH7ppxBBwatWsczi0cNY+Etti8a2otZRGVDg5yfWq9n4T1F9S0ma6kKrp6Mh4Hz5NegUVRFhFGFA9qWiigYUUUUxCUUUUwCiiigBBQaBQaBHnvw//wCRq8Uf9fo/lXoVee/D/wD5GrxR/wBfo/lXoVMBKKKKACikopgLSUqqWOFGauQWDPgvxSckiowctiokbSNhRmm3XhW31G5guLoAtD93IrdjgSIcCpaylI6oUrbmfe6Rb3ukyWDqBFImwjHaqlz4WsLvRYNNniDRwY2Z7EdDW3SiouzbYwj4ZhuNOms9RYXMcuM5GOlQQ+DrOO5E7fNIFCA+ijoK3L67Wxs3uJPuoKwI/GEMOgyalfI0Sq7AK3BIHel5j8iefwjZzWzx4Cszh93pinXfhmK8t4Vnk3TQ/ckx0qtqni1LTw3bavbwvLHO6KFXqAxxmuit5luLdJE5DDNAGHceEraS8W8hIiuvLEby4yWUdqdeeFLS7lguGAFzDGY1l77T1Fb9FAGTomgW2hxMlqMButa1FFABRRRQAUUUUAFFFFABVXUf+QbP/uVaqrqP/INn/wBygDA8Ef8AHvef9dq6muW8Ef8AHvef9dq6mgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDlPG33bL/rof5V0Wn/APIPh/3BXO+Nvu2X/XQ/yrotP/5B8P8AuCgCzRRRQAUUUUAFFFFABRRRQAUUUUAIaKWkoA8wvb+ZPF/iZlyWtYFaL2NVtSkey0vS7u0kPn3F4iykHnB6g11Unh+RPF11eCDzIL4BJvoKsJ4Rgim3sfNiRt6REdGpRVncJdjJ8GEx+LfEKAkq1yOPT5a7Sa1SQcDBrnfCmiXVhrWs3l2MC8nDxr6DGK6qmrpCaTMeW2eM9MioenWtxlDDBGaqz2QYZTg1akc06XYzaKe8Lxn5hTKswaa3CiiimISiiimAUUUUAIKDRRQI89+H/wDyNXij/r9H8q9BNee/D/8A5GrxR/1+j+VehGmAUUVJHA8p4FF7Ak3sRdasQ2by+wq9BYqgy/Jq2FCjgVm5nTCj1ZBBaJEOmT61PilorNu50qKWwUUUUhhQKKWgDB8abv8AhFbry87uOlYOqxW0vwyug6K7+QdvruwK7S9tEvbR4JBlXFZOl+GY7XSzZ38n2pRKzqSMYB7Urbg+hykU0Q+FWnrLwU8tSD2Oa7Xw8GGiw7+uKoaz4RttR0wWVufIj81ZSB32nNb9vCtvAsacBRimt2w6JElFFFABRRRQAUUUUAFFFFABRRRQAVV1H/kGz/7lWqq6j/yDZ/8AcoAwPBH/AB73n/XauprlvBH/AB73n/XaupoAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA5Txt92y/66H+VdFp/wDyD4f9wVzvjb7tl/10P8q6LT/+QfD/ALgoAs0UUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABSUtJigBkkSuPmFUZ7LHKVo0Yp3IlBMwmQqfmGKbWzLbpKORzWfNZsmSvIrRSRzSptbFaiggg4NFUYhRRRTASkpaACeAM0C1PPPh/wD8jV4o/wCv0fyr0REZzgDNcP8ADixZ/FXicvwPto/lXqUUCRj5RUuVjeFJvcpwWHeSryRqi/KKfRWbbZ0xgohS0mKWpLCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAqrqP/INn/wByrVVdR/5Bs/8AuUAYHgj/AI97z/rtXU1y3gj/AI97z/rtXU0AFFFFABRRRQBDdT/ZrWSbGdgziuFXx9rtxNMLHwzJPHG+3eH612mq/wDIKuP9w1i+Cv8Ajxuf+utAGR/wmvij/oUpf++6P+E18Uf9ClL/AN913tFAHBf8Jr4o/wChSl/77o/4TXxR/wBClL/33Xe0UAcF/wAJr4o/6FKX/vuj/hNfFH/QpS/9913tFAHBf8Jr4o/6FKX/AL7o/wCE18Uf9ClL/wB913tFAHBf8Jr4o/6FKX/vuj/hNfFH/QpS/wDfdd7RQBwX/Ca+KP8AoUpf++6P+E18Uf8AQpS/9913tFAHkXinxbr88dsbrw3JAEckEv1rQtPiJrsdpGq+GJGAUAHf1rd+IP8Ax4W/+8apWhP2OL/dFeJmeYVMJKKgk7kylYq/8LG17/oV5P8Av5R/wsbXv+hXk/7+VoZNGTXk/wBu4j+VEc7M/wD4WNr3/Qryf9/KP+Fja9/0K8n/AH8rQyaMmj+3cR/Kg52Z/wDwsbXv+hXk/wC/lH/Cxte/6FeT/v5Whk0ZNH9u4j+VBzsz/wDhY2vf9CvJ/wB/KP8AhY2vf9CvJ/38rQyaMmj+3cR/Kg52Z/8AwsbXv+hXk/7+Uf8ACxte/wChXk/7+VoZNGTR/buI/lQc7M//AIWNr3/Qryf9/KP+Fja9/wBCvJ/38rQyaMmj+3cR/Kg52Z//AAsbXv8AoV5P+/lH/Cxte/6FeT/v5Whk0ZNH9u4j+VBzsz/+Fja9/wBCvJ/38o/4WNr3/Qryf9/K0MmjJo/t3EfyoOdmf/wsfXv+hXk/7+Un/Cxte/6FeT/v5Wjk0ZNH9u4j+VBzszv+Fj69/wBCvJ/33S/8LH17/oV5P++60MmjJo/t3EfyoOdmd/wsbXv+hXk/7+Uh+Iuun/mVpP8AvutLJoyaP7dxH8q/EOcxZ/HWtycjwtID/v1ny+PvEMR/5FaTH+/XVZprKGHPNUs/xC+yvxMpR5tjkf8AhY2vf9CtJ/38o/4WNr3bwtJ/38ro57ENyhwaILILy/Jq/wDWCv8Ayow5ZXMKHx54hlP/ACK0gH+/WjD471uIc+FpCf8AfrYUbelOB5qHn+If2UdEY8p5v4f+IN5oGvazK+js73VxvZN33OOldH/wue7/AOhfb/vuuIvP+Ro1P/rt/Sn17scXNxTtuj7LL8mo4rDRqyk02dr/AMLnu/8AoX2/77o/4XPd/wDQvt/33XFUU/rU+x3f6vYf+Z/gdr/wue7/AOhfb/vuj/hdF3/0L7f991xVFH1qfYP9XsN/M/wO1/4XRd/9C+3/AH3R/wALou/+hfb/AL7riqKPrU+wf6vYf+Z/gdr/AMLou/8AoX2/77o/4XRd/wDQvt/33XFUUfWp9g/1ew/8z/A7X/hdF3/0L7f990f8Lou/+hfb/vuuKoo+tT7B/q9h/wCZ/gdr/wALou/+hfb/AL7o/wCF0Xf/AEL7f991xVFH1qfYP9XsP/M/wO1/4XRd/wDQvt/33R/wui7/AOhfb/vuuKoo+tT7B/q9h/5n+B2v/C6Lv/oX2/77o/4XRd/9C+3/AH3XFUUfWp9g/wBXsP8AzP8AA7X/AIXRd/8AQvt/33R/wui7/wChfb/vuuKoo+tT7B/q9h/5n+B2v/C6Lv8A6F9v++6P+F0Xf/Qvt/33XFUUfWp9g/1ew/8AM/wO1/4XRd/9C+3/AH3R/wALou/+hfb/AL7riqKPrU+wf6vYf+Z/gdr/AMLou/8AoX2/77o/4XRd/wDQvt/33XFUUfWp9g/1ew/8z/A7X/hdF3/0L7f991Dd/GS6ltJEOgsoZcE7+lchUVz/AMesn+7R9an2E+HsOl8T/A9j+HNz9t0eW5K7DK+7b6V2NcR8L/8AkWz9RXb16Cd1c+MnHlk49gooopkBRRRQBT1X/kFXH+4axfBX/Hjc/wDXWtrVf+QVcf7hrF8Ff8eNz/11oA6aiiigAoopMj1oAWiiigAooooAKKTcM4yM+lLQAUUUUAcf8Qf+Qfb/AO8apWn/AB5xf7oq78Qf+Qfb/wC8apWn/HnF/uivk8//AIkPQzmTUUUV82ZhRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAxKUdRRQOooA8kvP8AkaNT/wCu39KfTLz/AJGjU/8Art/Sn19lD4I+i/I/Ssl/3Cn8/wAwoooqz1wooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACorn/j1k/3alqK5/wCPWT/doE9j134X/wDItn6iu3riPhf/AMi2fqK7evZjsj8qq/xJerCiiiqMxcUYpaKAKWrELpNyTwAhrD8DyJJY3JQ7sS81ta3/AMgO7/65muc+HP8AyDbz/rtQB2OKMUtFAEc0iwwSSvwqKWJ9gK8uTxjeP4wiuhcRnSJn+U89K9G1uCe60me3tfvzI0f0BGM15p/wqO6TwYunR6nL9oj5U46c5pdbj05bHaXvjSwttbXSow8tyY1lKoM4U9DWJJ45mHjeDTvLIgaB5CCpz8tV18FarpviiLxBZt59y1nHaSRMcABR96rd34W1GTxbZaxGgOLZ4pkzwC1Ut/vJ6GnP4806Dw3cau27yYBluKh1T4haZpUMM1ySqSQrMOP4SM1z2p+AtUuNKvdIgOLWcYD5qHXPh3qV5iEL9qi+wrbjccbGAxmhW5VfcG9bItzeJ49U+IXhuSxuWFrfWckojz97kda9LA4rzPRvh1d6brHhq7aQsNLs3gkHqSc16aPuihpLYe4YoxS0UgOL+IsscWn2xkYLlz1+lUrOaI2UJDD7gp3xV/5Bdp/vn+VZ9j/yD4P9wV8XxJVdOpTt2M5mp5sf94UebH/eFUqK+U+sy7Gdy75sf94UebH/AHhVKqOp6xaaQitesyhuhVc0416knaKDU2/Nj/vCjzY/7wrndM8Sadq8rR2MjOVGSSpArUyPUfnTlWqwdpRswuXvNj/vCjzY/wC8KpZoyM4yM+man6zLsFy75sf94UebH/eFYeo61Y6UyLey+WZCAvvUr6lbR3EUDP8AvJvuD1q/bVbX5QNfzY/7wo82P+8Ko7hnGRn0zQWUdWA+pqPrMuwXL3mx/wB4UebH/eFUcj1H51A9/bpcmBpFEoXcVzzimsRN7ILmr5sf94UebH/eFYWm61aarNcxWjMWtm2Sblxz7VoZHqKcq9SLtJBcu+bH/eFHmx/3hVIEHoQfpRkHoQfoan6zLsFy75sf94UebH/eFczc+JbO21SaydiZIEDyADOAaii8aaNNKI4ppCxOMeWa2Uq7V1ANTq/Nj/vCjzY/7wrCtdcsLy5mt7eYNJD99fSqlz4psoVtXjbelxOYAcdGHWkqlZu3KGt7HUebH/eFHnR/3hVFWDKCpyDS1l9Zl2C5d86P+8KPOj/vCqVFH1mXYLl3zo/7woWWPcPmFUqVfvCj6zLsFzyq91C0HirVVMy5E+CKk/tC0/57L+tcvqH/ACOmtf8AXx/Spa+8pzfs4+i/I/WMgw8Z5bTk33/NnR/2haf89l/Wj+0LT/nsv61zlFXzs9v6pDudH/aFp/z2X9ajm1WyghaR5l2qMmsCqmp86bPj+4aam7kVMNGMHJPZHQWfiXTr4KYXIVujEHFWv7WsMEi5Q464rz7Rlng8PxSEnG3hcdKqQ3MxvohtKbnwwHcVu43k0uh5MaqhSpyqXblb8T0Sy8QafeLKy3CgRttOas/2pY7d32lNvrXl1uHS0vNpZCLgnGOtPmvriWO2VEZUYHeBTlDXQiniYqmnUTvb9bHp6alZOMpcIw9RVF/FGmJKyGXJU4JAPFcvoiSoCJCcdhWbFFPPfXMUUeVZzkmoVuZpnTVv7OEqd7yfqeix6xp8sIlS5TYehp/9p2IXd9oTHrXnEkX2C4t7N5GEQzjA61WSS6lscrI5VJWxn+L2p8t9UZ+2UfdnHVb+un+Z6VPrVjEoYTKwJ7Uw+IdMLOi3KF06ivP2vZJ9P+VWRldVYAVFbxhNXvy2dwA2nFPl01M3XTmlHZ/5P/I9Jg1iymTd56jnoak/tWwwT9pTA6+1eZ3V3PaWS3QLEb9uAKV5LnyY5BuMc43H2NHIyliKaVmm2lf79j0z+1LHZu+0Jt9aF1SxdSyXCMB3FecKbn7K6lm5Hyr61d0W6+0WEgCFSpIP1qJJpXOmlKE6ig002jqV8UaeWuCZl2QNtbg09fFGmmCSYuVSMZJINcFwbDUAo+bzlzVi6imHhi7LkkGLgVbSvb0OOMqnK3fZSf3N/wCR2cXirTJsbZdpYZUsDzU9nrlldNIonTdGcMPSuE0ywluYbeScbURBiptEz/bGqY+75ox+VEuVJ26GlFVHKnz7Sfp0bPQP7QtP+ey/rR/aFp/z2X9a5yiufnZ7H1SHc6P+0LT/AJ7L+tRXWoWgtJP3y/d96wagvP8Ajyl/3aOdilhYWep9EfCqWOfwyWiYMuRyK7nFec/Bb/kTz/vCvR696Pwo/Gq2lWXqxMUUtFUZBRRRQBQ1v/kB3f8A1zNc58Of+Qbef9dq6PW/+QHd/wDXM1znw5/5Bt5/12oA7KiiigAooooAz7vW7K0vktJZlE7kYTPPNO/tmx+1G2FwpmAJ2Z9K8s8RXcrfF7WYRki20lJox6NWLr0stv4Bh1W2nk/tCS9gV8N8wBbBFTF8z5f63HJWVz2rT9cs9SvJrW3lVpoP9YgPK1o15b4N3Q/FXXdhO2SKLcOw4r1KqVmri1TsFFFFABRRRQBwHxV/5Bdp/vn+VZ9j/wAg+D/cFaHxV/5Bdp/vn+VZ9j/yD4P9wV8LxR/Fp+hlUJ6KKK+QMwqhraK+kT7lBwvGRV+mTRLPEY35U9aqL5ZJjWjOB0y5k034Yaje2oCTxNKVbHPWs651TWtO8CWGqXN6Hlup4jkDopbkV6GdHtf7Nex2AQSZ3Ljrmlk0eym02OwmgR4IwNikdMdK9JYykpOTje8r/LsCty2Zxuo+KriHUNQNlOJPswX5AOmRTNA1rVr7U45XRljkIDE9K6238N6fbO7LCrNJ98sPvfWpLPQ7aymLwlsE52noKTxFBQcYw+Y3LQ5Tx/pa6trVlbPn/VF1wf4geKwrfVrhde0uxvGL3tk53j+VeoT2ENxdx3Ei5kjGFNQHQtPOp/bzbp9oPV9vJp0cbCFNU5q9k/vYSd0eY6X4l8QapfebC5AS9aN3xxsB6UXniHVr/wAQXtnFPtMVwAj9gvcV6QPDdlFM8tuvlbjuKLwufWuVuvA+pXE90iGGOG4nWUyqcSDHbNddPFYWcm+VLt94aGV4o8W6hp5ufsZYtaxhvMHQGoo2ubv4iWk0sxBk01JCexJFd7J4R06dGE6lt6BXB/iq3JoFjI0T+WFkiQIrqOdo7VlHG0IQtGOtmr/cLQ8+m1/U7G21i9iIK2dyibVXqDT5fFOpS2tzq9nLutZogsYA+6w6136aDYrDcRNErJcNukBHU0kPh/T4NLNhHAggOeNvTNR9cw+/Jr+mn6oEcVousa5Kr53Ro0JcBh95sVp+CdZubya4gvyVuEOHU+tdDbaBbW0LRozkEYBJ6fSn6dolrpru8K7pHOWduprGriKM4ySjvsGljldBCy/FjWRJhv8ARU4P1p2ixJ/wkWofIvAfHFbEXh6S18ZXGtW7AC4jWN1J7CtaLTbeGeSWNAGkzuOPWnVxEPs9YpfcPTU8sht307WbvXYGbYhP2hc/eJHFT6TNHc+BIrhDuP22R09jXpA0azEM0RiUpN98EdaxdT8J5ttPtNJCQ28NyZZV6ZBFdKx1OpZS0emvkkx82puaRk6Tbs2dzICc1dpkUYhhWNeijAp9eNJ3k2ZhRRRUjClX7wpKVfvCgDwbUP8AkdNa/wCvj+lS1FqH/I6a1/18f0qWv0On/Dj6L8j9h4d/5FlL5/mwoooqz3wpGUMpDDINLRQAxY1SPYoAUdqYLWIMGCDIqaii7J5IvoRG2iKkbRgnJ4pfIj2bdgx9KgvpLqKEvaBCVGTu9KoWWq3csP2m5EawD061ajJq6OadalCoqbWr8tDXSNYxhRgUiwojFlGCetRNfQoMluMZpq6jbs4UN171NpGvtKSdronkhjlxvUHFJ9ni2bNo2/SoZNRt4pCjtjHU9hUs02y3MiDdxkU/eDmpSu1rbcX7NFtxsGPpSG2iLlioyepqppOovfrN5igGNtvFWbi8itvvnJ9BQ1JOxFOpRnTVRbMc1tE0exkBXOcYpxhjMYQqNo6DFQyajbxRCR3GD05pv9p2/wAgLYLjIGaLSG6lFbtE/wBnj27dvFOSJI12ooA9qryalbxuVZuR1welOlv7eEKXcYbpzRaQ/aUVrdaEFvpccFxPIORM25hV3ylMZjI+U9qWNxJGHXoelOpNtvUqnThCNorT/MZ5YERROOMD2qvZWQtPMOctIcsat0UrsbpxclJrVbBRRRQaBUF5/wAeUv8Au1PUF5/x5S/7tBMtme9/Bb/kTz/vCvR684+C3/Inn/eFej19DH4UfiFf+LL1f5hRRRVGIUUUUAUNb/5Ad3/1zNc58Of+Qbef9dq6PW/+QHd/9czXOfDn/kG3n/XagDsqKKKACiiigDhNY8L3Z+IJ1u0gEyXMKW8wJxhBVr/hX1obx5JZPNtt3mC2I43DpXY0ULTYHrucP4X8N39l471nV7xdkN2qCJM5xiu4oooAKKKKACiiigDgPir/AMgu0/3z/Ks+x/5B8H+4K0Pir/yC7T/fP8qz7H/kHwf7gr4Xij+LT9DKoT0UUV8gZhRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABSr94UlKv3hQB4NqH/I6a1/18f0qWotQ/5HTWv+vj+lS1+h0/4cfRfkfsPDv/IspfP82FFFFWe+FFFFABRRRQBHOC1tKq9SpArJsbJm0Zre4XBx0rappUFSPWqUrKxz1KEak1J9mvvOQsIrme2vpJydqoVQfQ1LFaXF3ZxRRR7GBB3g10iWUMcLxKvyv1qSGFIECxjAFbOt1SPLp5W+VRnLo7/NnMvpty91OsykqSMf7VdE2Y7EKF5CYxU+B6UYrOVRytc76GDhQ5uV7mJoKTwTXInj2iR9y/So9RsbmTUmkBPlMfyrfwPSjGetP2j5uYz+oxdBUXLZ3Ofl0pmSBGXeok3GpJdMLa/A/lfuFQjOelbmB6UYo9rIbwFJ/en9xzU9jPHc3v8Ao/mCRQEOetPOlvI0CzpkLg4z0rosA9RRgelHtWQsup3u3/V7iIixoFQYA6CnUUVkemFFFFABRRRQAVBef8eUv+7U9QXn/HlL/u0Ey2Z738Fv+RPP+8K9Hrzj4Lf8ief94V6PX0MfhR+IV/4svV/mFFFFUYhRRRQBQ1v/AJAd3/1zNc58Of8AkG3n/Xauj1v/AJAd3/1zNc58Of8AkG3n/XagDsqKKKACiiigAooooAKKKKACiiigAooooA4D4q/8gu0/3z/Ks+x/5B8H+4K0Pir/AMgu0/3z/Ks+x/5B8H+4K+F4o/i0/QyqE9FFFfIGYUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUq/eFJSr94UAeDah/yOmtf9fH9KlqLUP+R01r/r4/pUtfodP+HH0X5H7Dw7/wAiyl8/zYUUUVZ74UUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABUF5/x5S/7tT1Bef8eUv+7QTLZnvfwW/5E8/7wr0evOPgt/yJ5/3hXo9fQx+FH4hX/iy9X+YUUUVRiFFFFAGfrrBdCuyxwBGa5H4e6tYxafeCS5RT53Qmus8Qx+d4dvYycBoiM15x4K8FW+oWd07XMqETY+U0Aelf23p3/P3H+dH9t6d/z9x/nXN/8K6tv+f2f86P+FdW3/P7P+dAHSf23p3/AD9x/nR/benf8/cf51zf/Curb/n9n/Oj/hXVt/z+z/nQB0n9t6d/z9x/nR/benf8/cf51zf/AArq2/5/Z/zo/wCFdW3/AD+z/nQB0n9t6d/z9x/nR/benf8AP3H+dc3/AMK6tv8An9n/ADo/4V1bf8/s/wCdAHSf23p3/P3H+dH9t6d/z9x/nXN/8K6tv+f2f86P+FdW3/P7P+dAHSf23p3/AD9x/nR/benf8/cf51zf/Curb/n9n/Oj/hXVt/z+z/nQBkfFHVbGXTbUR3KMd56H2qhY6lZDT4AbmPOwd6yfiz4Qh0jw0J4rqVnGSMnpXktvBcm3jP26YZXpur5rOsAsXOLcrWR62AyavmUXKk0rdz3f+07L/n5j/Oj+07L/AJ+Y/wA68M+z3P8Az/z/APfVH2e5/wCf+f8A76rwP7Dj/wA/PwPS/wBUcZ/Mj3P+07L/AJ+Y/wA6P7Tsv+fmP868M+z3P/P/AD/99UfZ7n/n/n/76o/sOP8Az8/AP9UcZ/Mj3P8AtOy/5+Y/zo/tOy/5+Y/zrwz7Pc/8/wDP/wB9UfZ7n/n/AJ/++qP7Dj/z8/AP9UcZ/Mj3P+07L/n5j/Oj+07L/n5j/OvDPs9z/wA/8/8A31R9nuf+f+f/AL6o/sOP/Pz8A/1Rxn8yPc/7Tsv+fmP86P7Tsv8An5j/ADrwz7Pc/wDP/P8A99UfZ7n/AJ/5/wDvqj+w4/8APz8A/wBUcZ/Mj3P+07L/AJ+Y/wA6P7Tsv+fmP868M+z3P/P/AD/99UfZ7n/n/n/76o/sOP8Az8/AP9UcZ/Mj3P8AtOy/5+Y/zo/tOy/5+Y/zrwz7Pc/8/wDP/wB9UfZ7n/n/AJ/++qP7Dj/z8/AP9UcZ/Mj3P+07L/n5j/Oj+07L/n5j/OvDPs9z/wA/8/8A31R9nuf+f+f/AL6o/sOP/Pz8A/1Rxn8yPc/7Tsv+fmP86P7Tsv8An5j/ADrwz7Pc/wDP/P8A99UfZ7n/AJ/5/wDvqj+w4/8APz8A/wBUcZ/Mj3P+07L/AJ+Y/wA6P7Tsv+fmP868M+z3P/P/AD/99UfZ7n/n/n/76o/sOP8Az8/AP9UcZ/Mj3P8AtOy/5+Y/zo/tOy/5+Y/zrwz7Pc/8/wDP/wB9UfZ7n/n/AJ/++qP7Dj/z8/AP9UcZ/Mj3P+07L/n5j/Oj+07L/n5j/OvDPs9z/wA/8/8A31R9nuf+f+f/AL6o/sOP/Pz8A/1Rxn8yPc/7Tsv+fmP86P7Tsv8An5j/ADrwz7Pc/wDP/P8A99UfZ7n/AJ/5/wDvqj+w4/8APz8A/wBUcZ/Mj3P+07L/AJ+Y/wA6UanZAj/SY/zrwv7Pc/8AP/P/AN9Uv2a5/wCf+f8A76o/sOP/AD8/AP8AVHGfzIS/uYf+Ew1h/MXa1xwc9eKf9tt/+ey/nV/4ceGLXXLnVjfu0hjnADHk9K7z/hW+keh/KvRrY/D4WfsZXvFL8jowfECyyisHOF3C6v8AM80+22//AD2X86Pttv8A89l/OvS/+Fb6R6H8qP8AhW+keh/Ksf7Ywvmdf+uNP/n0/vPNPttv/wA9l/Oj7bb/APPZfzr0v/hW+keh/Kj/AIVvpHofyo/tjC+Yf640/wDn0/vPNPttv/z2X86Pttv/AM9l/OvS/wDhW+keh/Kj/hW+keh/Kj+2ML5h/rjT/wCfT+880+22/wDz2X86Pttv/wA9l/OvS/8AhW+keh/Kj/hW+keh/Kj+2ML5h/rjT/59P7zzT7bb/wDPZfzo+22//PZfzr0v/hW+keh/Kj/hW+keh/Kj+2ML5h/rjT/59P7zzT7bb/8APZfzo+22/wDz2X869L/4VvpHofyo/wCFb6R6H8qP7YwvmH+uNP8A59P7zzT7bb/89l/Oj7bb/wDPZfzr0v8A4VvpHofyo/4VvpHofyo/tjC+Yf640/8An0/vPNPttv8A89l/Oj7bb/8APZfzr0v/AIVvpHofyo/4VvpHofyo/tjC+Yf640/+fT+880+22/8Az2X86Pttv/z2X869L/4VvpHofyo/4VvpHofyo/tjC+Yf640/+fT+880+22//AD2X86Pttv8A89l/OvS/+Fb6R6H8qP8AhW+keh/Kj+2ML5h/rjT/AOfT+880+22//PZfzo+22/8Az2X869L/AOFb6R6H8qP+Fb6R6H8qP7YwvmH+uNP/AJ9P7zzT7bb/APPZfzo+22//AD2X869L/wCFb6R6H8qP+Fb6R6H8qP7YwvmH+uNP/n0/vPNPttv/AM9l/Oobu7t2s5QJVJK8c16j/wAK30j0P5VBe/DrSY7GZ1zlVyOKFm+Fb6g+MKbVvZP7ztfgnIkng9tjA4YZxXpNeb/BrTk03QbuON2ZTNkZ7V6RX21NpwTR8PUlzzcu4UUUVZAUUUUAUNb/AOQHd/8AXM1znw5/5Bt5/wBdq6PW/wDkB3f/AFzNc58Of+Qbef8AXagDsqKKKACiiigAooooAKKKKACiiigAooooA8y+OH/In/g38q8Mtf8Aj1i/3RXufxw/5E/8G/lXhlr/AMesX+6K8vHfEj9B4S/hVPX9CWiiiuA+2CiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACgdaKB1pAdF8If+PjW/+vkfyr1GvLvhD/x8a3/18j+Veo18dm3++T+X5I/Dcw/3yr/if5hRRRXlnCFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVX1D/kG3H+4asVX1D/kG3H+4aa3QGr8LP8AkD3X/XWu7rhPhZ/yB7r/AK613dfsdH+FH0X5HSFFFFagFFFFAFDW/wDkB3f/AFzNc58Of+Qbef8AXauj1v8A5Ad3/wBczXOfDn/kG3n/AF2oA7KiiigAqv8Ab7X7T9n+0R+d08vcN35Ut7cC1sJ7hukUbP8AkM18/wAWrW8njODxkt2wilky8eTjjikn71gekbn0PTfNTfs3Dd6Z5rz688Y3uo+Nl0DSz5JS0juzLnIYMOlYclzr0fxQtkfUsQ/ZZZDHjqQKfUOh6/RXm1149urfwVfarjMtupIXPPWotf8AHk1nawz290FlWzS5eDH3sjOKaV1cOtj0SXUbaG/is5JVE8qlkQnkgVarxRNYPiL4m+C9X+aL7Tp0ztHu6HIr2pfuii1gFooopAeZfHD/AJE/8G/lXhlr/wAesX+6K9z+OH/In/g38q8Mtf8Aj1i/3RXl474kfoPCX8Kp6/oS0UUVwH2wUySVIk3SMFHvT6xfFBI0psAnpwDVQjzSSMMRVdGjKouiua0cyS/cOaWWRYk3PwKxY7mWLT4/3ZtnZQFYnOTiq1te3UyywXj7+CQav2bb0ON4+MUlJav7v8zo4ZVniEkfKnpT65Y3d3bWFr5EnlRc7264p95q92bwWtm24iMOX9c0/Yu9kQszpqCc076fO66HSk4BJ7VFHdQy7vLcHb1waq2U882nSG6XDhDn34rmbJ3sY/tAz5U5Ic5+7SjTvfUuvjnScGo6NNvy/q52cciyrlORT65cahcLaqlk+0s+d1PbUdQjs5BM2yQMAr+op+xkTHM6VtU9r+R0tISFGWOB71zsN5fLbyvNc/eX93x3qH7deXFncQzv8yxlt3tR7F9xvMoKKfK9To5bqKIKSchuhFSg5ANctaSTx6LaHzdxZOPbmphrE8cn2J5c3BHynFDovZEwzKHKpTVrpfitjpKK5canqU0w8knCHDL61Pd6jeI0Ts/kR/x96Xsn3L/tKk05crsvI3LidLa3eaU4RBk1EmoQOisG4YZGaqavIJNGbncrpgmsucbLqzEaGTEI+UGnGmmtScRjJ0qlorSy/FnRNeQpt3uF3dMmnPcRxjLHtmuZ1+3a7u9Oi2GJmBO3PSpbW48/UI4Zch0wpX2p+yXKmZ/2hL2sqbXVJP1Sf6nQW9zHcqTEchTg1NWJ4eY+bfr2WcgVt1lOPLKx6GFrOtRU35/nYKKKKk6QoHWigdaQHRfCH/j41v8A6+R/KvUa8u+EP/Hxrf8A18j+Veo18dm3++T+X5I/Dcw/3yr/AIn+YUUUV5ZwhRRSbl7so/GgBaKAQehzRQAUUUUAFFJuX+8v50tABRRTWkVCNzAZOBk0AOopCQBkmhXDrlTketAC0UUUAFFFRR3MMjFUkUkdeaAJaKTcvZlP40F1A+8v4GgBaKiguYrkMYXDhTg47GpaGmtwCiiigAqvqH/INuP9w1YqvqH/ACDbj/cNNboDV+Fn/IHuv+utd3XCfCz/AJA91/11ru6/Y6P8KPovyOkKKKK1AKKKKAKGt/8AIDu/+uZrnPhz/wAg28/67V0et/8AIDu/+uZrnPhz/wAg28/67UAdlRRRQBU1OxGo2ElqxKrIpVsehGK5ZvhloreE/wCw9jLFjhx94c5612lFAHGzfDnTzqEV9BNNHcpAkBcNjKqMCpJ/AVpNrtnqhml861haEfNwwPXNXNQ8VRWviNNHhAe4wHceinvToPGWl3WvNo0LsbwKxCkcHFC7oDE1H4W6ZqK3KPPOkc4wY1fA/Km6p8LdN1KYO8kg/wBFW2OG/hAxW3onimLVdev9JZdtxZAFx9a6KndpWDrc4vTvhvpunalo95E8hfSrdoIgT1BOea7QcCiii4BRRRSA8y+OH/In/g38q8Mtf+PWL/dFe5/HD/kT/wAG/lXhlr/x6xf7ory8d8SP0HhL+FU9f0JaKKK4D7YKr3dnHeR7Js7farFFCbTuiZRU4uMloyvPZxXECxSZ2qMD2qvBo1tAxKlySMcmtCimpSWiZlLD0pS5pR1M2TRLaWNUZn2r0GafLo9tLg/MpAxleuKuSMUjLAZxWemsxveG1UfvFGSMdqtOb2MKlPC07KaWuhditlhgMQJKkYyar/2RbfYWtMHyz+dW1kUpuLAD60GWMdZFH41F2dDp0mrNLa3yKLaJbNbrDllVTkEHmoL3SCYAkDEqOu481rb1AyWAH1o3DaSDkYzxVKck7mM8JQnFxtYxbLRX8yX7U2Y3GFAPSrkOj20G/aWO9dpye1NbWoEuIonyDK21eOtaG8Bck4HvVSlPqZUKGF2hZtFGLR7aKMIpYqvQE9KVtItnvluyD5q4xV3zExneuPXNJ5iY++v51HNLudH1ehZLlX/DFSXSbeWbzMshznCnGaSfR7e4YGUucds8Vd8xMZ3rj1zR5ibc71x65o5pA8PQd04rUp6jZ/aNLkgi4bZhajstOASF58+bEgXitBXVx8rBh7GnUczSsN4enKftGulivPZxXFzFPJnfD92oH0uFbl7uMHziuB6VfopczRUqFOTu15/PuZ+lWRs1mLfelfca0KKKJScndjpUo0oKEdkFFFFI1CgdaKB1pAdF8If+PjW/+vkfyr1GvLvhD/x8a3/18j+Veo18dm3++T+X5I/Dcw/3yr/if5hRRRXlnCFee+NtPNnd2r293cL586qwD8YJr0KsPxBoJ1mS2O4r5Mivx3xXXg6qpVVKT0C7szL1jxEPC17p+j20U1zLcqSp6nANWbfxWZNU1CyeJg9nbrMePWrGpeHft3i7TdX3kCzRl2+uazNX8Gz3OuX19Z3TxfboRC4H8IHet4vCzilLe2r87/5Fe7cfc+N4oYVAG2ZuRu9K1tC1waxbTfIQ8Q+ZscHjtWBN4C32kJMhlmiwMt/EBXS6PpY0+3kAUIZBgqOgqa31VU/3e4nboeb39xI76hcR3kxvYQTDCr/eYHgYrpLLxddiCxsTbSG9a186UleBgcj61p6V4StbHUpbqZBK7HILDpzUOreFJZ9aXUrG5aJvKMZjHTnvXTPEYep7jWi2fnbYp2bKz+PF/s+CX7PJG8v3mccJ9ap63rkNwNHuJGm3y3YRPKPBOO/tV8eCyuiraPIZ2I+ct3qtH4BeK006I3TyG0uzcc+npShLBxldO2/6i93oUtF8Q3+oXGuRXufs9tOUBTqOKtaL4vt7G3trabeIpiRE0h5NXbTwhJY3epeVKWiv5DI2exxVLV/h7/aGiWVtHM0c9oSVcdeat1MJOVpaJ2/L/MWjZM/jwLaEmMpN5xQBh/D2NSv45X+zHmWBw0bBWcj5eadfeCIriG1ZGzLCqhh/ewKll8LF9PeBVG2T70fYVi3grKyA19L1T7dpP2rcrcZyvSuJ8N6c+oeG9YnubqYSJdSshV+gzwK67QPD40bR/sRlZwQevasXwVp1zb6XqtheKUM1xIUP+yTxSpyhCFR031VvS4fkZekWTQ+HLjUDeXDSrwNz8c1m6VeXUd9o8mnXE101zclLpS24Iv8ASu8t/Dgg0KbT95IkI5qTw/4ds9AslVIl3qSS+Oa2eMppTb1benpYH5eZk+DQ1pqV9as7NvmaT5jnHPSuxrkPBtvcyXuoXl2hQ/aHVB6rnrXX1wYz+Mwl8TCiiiuQQVX1D/kG3H+4asVX1D/kG3H+4aa3QGr8LP8AkD3X/XWu7rhPhZ/yB7r/AK613dfsdH+FH0X5HSFFFFagFFFFAFDW/wDkB3f/AFzNc58Of+Qbef8AXauj1v8A5Ad3/wBczXOfDn/kG3n/AF2oA7KiiigAooooA8sTn9orUBN/qv7Lixn15qob2xtvjnYJC+Fa2l38d+K7fUfCgn8VLrds22dlWOTPdRWhJ4a01rv7WLaP7SAdsm3kfjRHTX1HLX8DifDbD/hcviLyu4TdivTa5jQPCg0rxJqOsStunvgAwzwMV09JKysDd2FFFFMQUUUUAeZfHD/kT/wb+VeGWv8Ax6xf7or3P44f8if+Dfyrwy1/49Yv90V5eO+JH6Dwl/Cqev6EtFFFcB9sFFFFABRRRQAVg26BvF1wpHBgHat6mCKMSGQIA5GC2OauMuW/mc1eh7Zwd/hd/wAzkrlrhb2XTmLMJZPMUjsBUc3lNdX0UzsXRR5YB710x02N9SW8b76qVH40kGk28V5NcFA7S/3u1b+2jY8aeW1ZS0enN1/ls/1ZzkrXbypFdM0cHlja3vXQ6Upisfmk8xQMg+1XWgicAPGrAdMinKiqu1VAX0FZTqcytY78PgXRquo5Xuc015b6nrEbSMEjt2yvHU1c8RNMbKEWxOxnG4j0rVFnbA5EEYP0qRo0ZdrKCPQ0e0V00tgWCm6VSMpe9LqjlFhMWm3AiuDIhdcD0q1NZCPTY9u4tJwea3hbQhSBEoB6jHWnmNCACowOg9KftXczjlsUrN9Lfic7cQrbvDFLuW2Kgsc96ozGRdO1IQFvI81REfauveKOQYkRWHoRTfs8OzZ5S7D/AA44pxrW3QquWud1GVlZ/lb7upX0q0S0sYwmeVBOTV2kAAGBwBS1i3d3PUp01TgoR2QUUUUjQKKKKACiiigAoHWigdaQHRfCH/j41v8A6+R/KvUa8u+EP/Hxrf8A18j+Veo18dm3++T+X5I/Dcw/3yr/AIn+YUUUV5ZwhRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUgAHQAfhS0UAFFFFACBQv3QB9BS0UUAFFFFABVfUP+Qbcf7hqxVfUP+Qbcf7hprdAavws/5A91/wBda7uuE+Fn/IHuv+utd3X7HR/hR9F+R0hRRRWoBRRRQBQ1v/kB3f8A1zNc58Of+Qbef9dq6PW/+QHd/wDXM1znw5/5Bt5/12oA7KiiigAooooAKKKKACiiigAooooAKKKKAPMvjh/yJ/4N/KvDLX/j1i/3RXufxw/5E/8ABv5V4Za/8esX+6K8vHfEj9B4S/hVPX9CWiiiuA+2CiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACgdaKB1pAdF8If8Aj41v/r5H8q9Rry74Q/8AHxrf/XyP5V6jXx2bf75P5fkj8NzD/fKv+J/mFFFFeWcIUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABVfUP+Qbcf7hqxVfUP+Qbcf7hprdAavws/5A91/wBda7uuE+Fn/IHuv+utd3X7HR/hR9F+R0hRRRWoBRRRQBQ1v/kB3f8A1zNc58Of+Qbef9dq6jU4HudLuIYxlnTAFeYaYvjrQGuoLPQlmieUsrmQDIoA9ZorzT+2/iH/ANC4v/f0Uf238Q/+hcX/AL+igD0uivNP7b+If/QuL/39FH9t/EP/AKFxf+/ooA9LorzT+2/iH/0Li/8Af0Uf238Q/wDoXF/7+igD0uivNP7b+If/AELi/wDf0Uf238Q/+hcX/v6KAPS6K80/tv4h/wDQuL/39FH9t/EP/oXF/wC/ooA9LorzT+2/iH/0Li/9/RR/bfxD/wChcX/v6KAG/HD/AJE/8G/lXhlr/wAesX+6K9X8W2Hj3xZpYsrjQViXnLeYDXHJ8KvGaIFFhwBj74rhxVGdRpxPruH80w2BpzjXdrvsc9RXRf8ACrPGn/Ph/wCPij/hVnjT/nw/8fFcn1Sr2Ppv9ZMu/mf3M52iui/4VZ40/wCfD/x8Uf8ACrPGn/Ph/wCPij6pV7B/rJl38z+5nO0V0X/CrPGn/Ph/4+KP+FWeNP8Anw/8fFH1Sr2D/WTLv5n9zOdorov+FWeNP+fD/wAfFH/CrPGn/Ph/4+KPqlXsH+smXfzP7mc7RXRf8Ks8af8APh/4+KP+FWeNP+fD/wAfFH1Sr2D/AFky7+Z/cznaK6L/AIVZ40/58P8Ax8Uf8Ks8af8APh/4+KPqlXsH+smXfzP7mc7RXRf8Ks8af8+H/j4o/wCFWeNP+fD/AMfFH1Sr2D/WTLv5n9zOdorov+FWeNP+fD/x8Uf8Ks8af8+H/j4o+qVewf6yZd/M/uZztFdF/wAKs8af8+H/AI+KP+FWeNP+fD/x8UfVKvYP9ZMu/mf3M52iui/4VZ40/wCfD/x8Uf8ACrPGn/Ph/wCPij6pV7B/rJl38z+5nO0V0X/CrPGn/Ph/4+KP+FWeNP8Anw/8fFH1Sr2D/WTLv5n9zOdorov+FWeNP+fD/wAfFH/CrPGn/Ph/4+KPqlXsH+smXfzP7mc7RXRf8Ks8af8APh/4+KP+FWeNP+fD/wAfFH1Sr2D/AFky7+Z/cznaB1rov+FWeNP+fD/x8Uo+FnjQH/jw/wDHxR9Uq9h/6yZd/M/uYnwh/wCPjW/+vkfyr1GuA8MeAvGvhdrtrfTBMbqTzGy4G32rf+wePf8AoCD/AL+CvnMwyXGV8TKpCOjt1XY/LcZJVcROpHZts6Ciuf8AsHj3/oCD/v4KPsHj3/oCD/v4K4P9X8f/ACr70c3IzoKK5/7B49/6Ag/7+Cj7B49/6Ag/7+Cj/V/H/wAq+9ByM6Ciuf8AsHj3/oCD/v4KPsHj3/oCD/v4KP8AV/H/AMq+9ByM6Ciuf+wePf8AoCD/AL+Cj7B49/6Ag/7+Cj/V/H/yr70HIzoKK5/7B49/6Ag/7+Cj7B49/wCgIP8Av4KP9X8f/KvvQcjOgorn/sHj3/oCD/v4KPsHj3/oCD/v4KP9X8f/ACr70HIzoKK5/wCwePf+gIP+/go+wePf+gIP+/go/wBX8f8Ayr70HIzoKK5/7B49/wCgIP8Av4KPsHj3/oCD/v4KP9X8f/KvvQcjOgorn/sHj3/oCD/v4KPsHj3/AKAg/wC/go/1fx/8q+9ByM6Ciuf+wePf+gIP+/go+wePf+gIP+/go/1fx/8AKvvQcjOgorn/ALB49/6Ag/7+Cj7B49/6Ag/7+Cj/AFfx/wDKvvQcjOgorn/sHj3/AKAg/wC/go+wePf+gIP+/go/1fx/8q+9ByM6Cq+of8g24/3DWP8AYPHv/QEH/fwUyfS/Hk0DxHRQAwx/rBTXD+Pv8K+9ByM7D4Wf8ge6/wCutd3XI/DzRr/RtGkj1OLyppH3Fc5xXXV+jU4uMIxfRI2CiiitACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAP/Z)"
      ],
      "metadata": {
        "id": "KAMj-BI3n8Bc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Print other metrics"
      ],
      "metadata": {
        "id": "hqG5ZMxMn_wE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "L1kU1Vbl0X9G",
        "outputId": "9e52c7eb-94f6-433f-878f-159f75704f50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy 0.9304851556842868\n",
            "precision 0.9241245136186771\n",
            "recall 0.8928571428571429\n"
          ]
        }
      ],
      "source": [
        "# accuracy\n",
        "print(\"accuracy\", metrics.accuracy_score(y_test, y_pred_rbf1))\n",
        "\n",
        "# precision\n",
        "print(\"precision\", metrics.precision_score(y_test, y_pred_rbf1))\n",
        "\n",
        "# recall/sensitivity\n",
        "print(\"recall\", metrics.recall_score(y_test, y_pred_rbf1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwezJ1bW0X9G"
      },
      "source": [
        "## Optimising for Other Evaluation Metrics\n",
        "\n",
        "In this case, we had optimised (tuned) the model based on overall accuracy, though that may not always be the best metric to optimise. For example, if you are concerned more about catching all spams (positives), you may want to maximise TPR or sensitivity/recall. If, on the other hand, you want to avoid classifying hams as spams (so that any important mails don't get into the spam box), you would maximise the TNR or specificity.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "scrolled": true,
        "id": "liu-dD-X0X9G",
        "outputId": "bc2764a6-6b53-44c1-b223-cedadfc4a1ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Tuning hyper-parameters for accuracy\n",
            " The highest accuracy score is 0.9307453416149067 at C = {'C': 10}\n",
            "\n",
            "\n",
            "# Tuning hyper-parameters for precision\n",
            " The highest precision score is 0.9373311086391073 at C = {'C': 0.1}\n",
            "\n",
            "\n",
            "# Tuning hyper-parameters for recall\n",
            " The highest recall score is 0.8987270122310326 at C = {'C': 10}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# specify params with C as list [0.1, 1, 10, 100, 1000]\n",
        "params = {\"C\": [0.1, 1, 10, 100, 1000]}\n",
        "\n",
        "# specify scores/metrics in an iterable i.e.['accuracy', 'precision', 'recall']\n",
        "scores = ['accuracy', 'precision', 'recall']\n",
        "\n",
        "# iterate through scores\n",
        "for score in scores:\n",
        "    print(\"# Tuning hyper-parameters for {}\".format(score))\n",
        "    \n",
        "    # set up GridSearch for score metric with estimator, params, cv, scoring as scores and return_train_score=True\n",
        "    clf = GridSearchCV(SVC(),\n",
        "                       params,\n",
        "                       cv=folds,\n",
        "                       scoring=score,\n",
        "                       return_train_score=True)\n",
        "    \n",
        "    \n",
        "    # fit model on training data\n",
        "    clf.fit(X_train, y_train)\n",
        "    \n",
        "    \n",
        "    print(\" The highest {0} score is {1} at C = {2}\".format(score, clf.best_score_, clf.best_params_))\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34aQGWNO0X9H"
      },
      "source": [
        "Thus, you can see that the optimal value of the hyperparameter varies significantly with the choice of evaluation metric.\n",
        "\n",
        "\n",
        "We have 3 most important hyperparameters to optimise in SVM - \n",
        "- The choice of kernel (linear, rbf etc.)\n",
        "- C\n",
        "- gamma\n",
        "\n",
        "To know what these parameters mean check out below reference\n",
        "\n",
        "Reference doc: https://towardsdatascience.com/svm-hyperparameters-explained-with-visualizations-143e48cb701b\n",
        "\n",
        "Lets do optimize these parameter values and check our model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "dzBkfYm30X9H"
      },
      "outputs": [],
      "source": [
        "# Get list of all available kernels in SVM in a list i.e. ['Polynomial', 'RBF', 'Sigmoid','Linear']\n",
        "kernels = ['Polynomial', 'RBF', 'Sigmoid', 'Linear']\n",
        "\n",
        "# A function which returns the corresponding SVC model for different kernals\n",
        "def getClassifier(ktype):\n",
        "    if ktype == 0:\n",
        "        # Polynomial kernal\n",
        "        return SVC(kernel='poly', degree=8, gamma=\"auto\")\n",
        "    elif ktype == 1:\n",
        "        # Radial Basis Function kernal\n",
        "        return SVC(kernel='rbf', gamma=\"auto\")\n",
        "    elif ktype == 2:\n",
        "        # Sigmoid kernal\n",
        "        return SVC(kernel='sigmoid', gamma=\"auto\")\n",
        "    elif ktype == 3:\n",
        "        # Linear kernal\n",
        "        return SVC(kernel='linear', gamma=\"auto\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ikUEEIxi0X9H",
        "outputId": "43cff9f4-decd-4fbe-bcaa-137765b4ea96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation: Polynomial kernel\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.99      0.79       849\n",
            "           1       0.91      0.20      0.33       532\n",
            "\n",
            "    accuracy                           0.69      1381\n",
            "   macro avg       0.79      0.60      0.56      1381\n",
            "weighted avg       0.76      0.69      0.62      1381\n",
            "\n",
            "Evaluation: RBF kernel\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.96      0.94       849\n",
            "           1       0.93      0.89      0.90       532\n",
            "\n",
            "    accuracy                           0.93      1381\n",
            "   macro avg       0.93      0.92      0.92      1381\n",
            "weighted avg       0.93      0.93      0.93      1381\n",
            "\n",
            "Evaluation: Sigmoid kernel\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.91      0.92       849\n",
            "           1       0.86      0.87      0.86       532\n",
            "\n",
            "    accuracy                           0.90      1381\n",
            "   macro avg       0.89      0.89      0.89      1381\n",
            "weighted avg       0.90      0.90      0.90      1381\n",
            "\n",
            "Evaluation: Linear kernel\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.95      0.94       849\n",
            "           1       0.92      0.89      0.91       532\n",
            "\n",
            "    accuracy                           0.93      1381\n",
            "   macro avg       0.93      0.92      0.92      1381\n",
            "weighted avg       0.93      0.93      0.93      1381\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# import classification_report\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Iterate in the range of 4\n",
        "for i in range(4):\n",
        "    \n",
        "    # Use getClassifier() function for different ith value to get SVC model using different kernal\n",
        "    svclassifier = getClassifier(i)\n",
        "    \n",
        "    # fit above model\n",
        "    svclassifier.fit(X_train, y_train)\n",
        "\n",
        "    # Make prediction on test data\n",
        "    y_pred = svclassifier.predict(X_test)    \n",
        "    \n",
        "    # Check which kernal is used as per kernels list\n",
        "    print(\"Evaluation:\", kernels[i], \"kernel\")\n",
        "    \n",
        "    # Evaluate our model using classification_report\n",
        "    print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zxj2VmO0X9H"
      },
      "source": [
        "From above you can see thta rbf krnals gives better prediction result\n",
        "\n",
        "lets do gridsearch and confirm value of C, gamma and kernel.\n",
        "\n",
        "`\n",
        "C= [0.1,1, 10, 100]\n",
        "gamma= [1,0.1,0.01,0.001]\n",
        "kernel= ['rbf', 'poly', 'sigmoid']`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "n7fo6aQq0X9H",
        "outputId": "01428397-1493-467c-c3b2-dbfec5e9b6fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.7s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.7s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.7s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.7s\n",
            "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.7s\n",
            "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   0.3s\n",
            "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   0.2s\n",
            "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   0.3s\n",
            "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   0.4s\n",
            "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=   0.3s\n",
            "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.3s\n",
            "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=   0.3s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.6s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.6s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.6s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.6s\n",
            "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.6s\n",
            "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.3s\n",
            "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.3s\n",
            "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.3s\n",
            "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.3s\n",
            "[CV] END ......................C=0.1, gamma=0.1, kernel=poly; total time=   0.3s\n",
            "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.3s\n",
            "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.3s\n",
            "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.3s\n",
            "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.3s\n",
            "[CV] END ...................C=0.1, gamma=0.1, kernel=sigmoid; total time=   0.3s\n",
            "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.4s\n",
            "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.4s\n",
            "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.4s\n",
            "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.4s\n",
            "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.4s\n",
            "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.4s\n",
            "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.4s\n",
            "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.4s\n",
            "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.4s\n",
            "[CV] END .....................C=0.1, gamma=0.01, kernel=poly; total time=   0.5s\n",
            "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.4s\n",
            "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.4s\n",
            "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.4s\n",
            "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.4s\n",
            "[CV] END ..................C=0.1, gamma=0.01, kernel=sigmoid; total time=   0.4s\n",
            "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.6s\n",
            "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.6s\n",
            "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.6s\n",
            "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.6s\n",
            "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.6s\n",
            "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=   0.4s\n",
            "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=   0.4s\n",
            "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=   0.4s\n",
            "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=   0.5s\n",
            "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=   0.4s\n",
            "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=   0.6s\n",
            "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=   0.6s\n",
            "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=   0.6s\n",
            "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=   0.6s\n",
            "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=   0.6s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.8s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.8s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.8s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.8s\n",
            "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.8s\n",
            "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   0.4s\n",
            "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   0.3s\n",
            "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   0.3s\n",
            "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   0.3s\n",
            "[CV] END ..........................C=1, gamma=1, kernel=poly; total time=   0.4s\n",
            "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.3s\n",
            "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END .......................C=1, gamma=1, kernel=sigmoid; total time=   0.3s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.5s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.5s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.5s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.5s\n",
            "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.5s\n",
            "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.3s\n",
            "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.2s\n",
            "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.2s\n",
            "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.3s\n",
            "[CV] END ........................C=1, gamma=0.1, kernel=poly; total time=   0.3s\n",
            "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END .....................C=1, gamma=0.1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.2s\n",
            "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.2s\n",
            "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.3s\n",
            "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.3s\n",
            "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.3s\n",
            "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.4s\n",
            "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.4s\n",
            "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.4s\n",
            "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.4s\n",
            "[CV] END .......................C=1, gamma=0.01, kernel=poly; total time=   0.4s\n",
            "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END ....................C=1, gamma=0.01, kernel=sigmoid; total time=   0.3s\n",
            "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.3s\n",
            "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.4s\n",
            "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.3s\n",
            "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.3s\n",
            "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.4s\n",
            "[CV] END ......................C=1, gamma=0.001, kernel=poly; total time=   0.5s\n",
            "[CV] END ......................C=1, gamma=0.001, kernel=poly; total time=   0.5s\n",
            "[CV] END ......................C=1, gamma=0.001, kernel=poly; total time=   0.4s\n",
            "[CV] END ......................C=1, gamma=0.001, kernel=poly; total time=   0.5s\n",
            "[CV] END ......................C=1, gamma=0.001, kernel=poly; total time=   0.5s\n",
            "[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=   0.4s\n",
            "[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=   0.4s\n",
            "[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=   0.4s\n",
            "[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=   0.4s\n",
            "[CV] END ...................C=1, gamma=0.001, kernel=sigmoid; total time=   0.4s\n",
            "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.8s\n",
            "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.8s\n",
            "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.8s\n",
            "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.8s\n",
            "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.8s\n",
            "[CV] END .........................C=10, gamma=1, kernel=poly; total time=   0.5s\n",
            "[CV] END .........................C=10, gamma=1, kernel=poly; total time=   0.3s\n",
            "[CV] END .........................C=10, gamma=1, kernel=poly; total time=   0.4s\n",
            "[CV] END .........................C=10, gamma=1, kernel=poly; total time=   0.3s\n",
            "[CV] END .........................C=10, gamma=1, kernel=poly; total time=   0.3s\n",
            "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.3s\n",
            "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END ......................C=10, gamma=1, kernel=sigmoid; total time=   0.3s\n",
            "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.6s\n",
            "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.6s\n",
            "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.6s\n",
            "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.6s\n",
            "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.6s\n",
            "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   0.3s\n",
            "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   0.2s\n",
            "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   0.2s\n",
            "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   0.3s\n",
            "[CV] END .......................C=10, gamma=0.1, kernel=poly; total time=   0.3s\n",
            "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END ....................C=10, gamma=0.1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.2s\n",
            "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.2s\n",
            "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.2s\n",
            "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.2s\n",
            "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.2s\n",
            "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.4s\n",
            "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.3s\n",
            "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.3s\n",
            "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.3s\n",
            "[CV] END ......................C=10, gamma=0.01, kernel=poly; total time=   0.3s\n",
            "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.1s\n",
            "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.1s\n",
            "[CV] END ...................C=10, gamma=0.01, kernel=sigmoid; total time=   0.1s\n",
            "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.2s\n",
            "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.2s\n",
            "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.2s\n",
            "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.2s\n",
            "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.2s\n",
            "[CV] END .....................C=10, gamma=0.001, kernel=poly; total time=   0.5s\n",
            "[CV] END .....................C=10, gamma=0.001, kernel=poly; total time=   0.5s\n",
            "[CV] END .....................C=10, gamma=0.001, kernel=poly; total time=   0.4s\n",
            "[CV] END .....................C=10, gamma=0.001, kernel=poly; total time=   0.4s\n",
            "[CV] END .....................C=10, gamma=0.001, kernel=poly; total time=   0.4s\n",
            "[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=   0.3s\n",
            "[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END ..................C=10, gamma=0.001, kernel=sigmoid; total time=   0.3s\n",
            "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.8s\n",
            "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.8s\n",
            "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.8s\n",
            "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.8s\n",
            "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.8s\n",
            "[CV] END ........................C=100, gamma=1, kernel=poly; total time=   0.5s\n",
            "[CV] END ........................C=100, gamma=1, kernel=poly; total time=   0.4s\n",
            "[CV] END ........................C=100, gamma=1, kernel=poly; total time=   0.6s\n",
            "[CV] END ........................C=100, gamma=1, kernel=poly; total time=   0.5s\n",
            "[CV] END ........................C=100, gamma=1, kernel=poly; total time=   0.4s\n",
            "[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=   0.3s\n",
            "[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time=   0.3s\n",
            "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.5s\n",
            "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.5s\n",
            "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.5s\n",
            "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.5s\n",
            "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.5s\n",
            "[CV] END ......................C=100, gamma=0.1, kernel=poly; total time=   0.3s\n",
            "[CV] END ......................C=100, gamma=0.1, kernel=poly; total time=   0.2s\n",
            "[CV] END ......................C=100, gamma=0.1, kernel=poly; total time=   0.3s\n",
            "[CV] END ......................C=100, gamma=0.1, kernel=poly; total time=   0.5s\n",
            "[CV] END ......................C=100, gamma=0.1, kernel=poly; total time=   0.3s\n",
            "[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END ...................C=100, gamma=0.1, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.2s\n",
            "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.2s\n",
            "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.3s\n",
            "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.2s\n",
            "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.2s\n",
            "[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=   0.3s\n",
            "[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=   0.3s\n",
            "[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=   0.3s\n",
            "[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=   0.3s\n",
            "[CV] END .....................C=100, gamma=0.01, kernel=poly; total time=   0.3s\n",
            "[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=   0.1s\n",
            "[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=   0.1s\n",
            "[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=   0.1s\n",
            "[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=   0.1s\n",
            "[CV] END ..................C=100, gamma=0.01, kernel=sigmoid; total time=   0.1s\n",
            "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.2s\n",
            "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.2s\n",
            "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.2s\n",
            "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.2s\n",
            "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.2s\n",
            "[CV] END ....................C=100, gamma=0.001, kernel=poly; total time=   0.4s\n",
            "[CV] END ....................C=100, gamma=0.001, kernel=poly; total time=   0.4s\n",
            "[CV] END ....................C=100, gamma=0.001, kernel=poly; total time=   0.4s\n",
            "[CV] END ....................C=100, gamma=0.001, kernel=poly; total time=   0.4s\n",
            "[CV] END ....................C=100, gamma=0.001, kernel=poly; total time=   0.5s\n",
            "[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=   0.2s\n",
            "[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=   0.2s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(estimator=SVC(),\n",
              "             param_grid={'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001],\n",
              "                         'kernel': ['rbf', 'poly', 'sigmoid']},\n",
              "             return_train_score=True, verbose=2)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# Create a dictionary param_grid for C, gamma and kernel values given below\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'gamma':[1, 0.1, 0.01, 0.001], 'kernel': ['rbf', 'poly', 'sigmoid']}\n",
        "\n",
        "# Initialise GridSearchCV with SVC(), param_grid, refit=True and verbose=2 and  return_train_score=True\n",
        "grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=2, return_train_score=True)\n",
        "\n",
        "# fit Gridsearch on X_train and y_train\n",
        "grid.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "w9jS3Zgj0X9H",
        "outputId": "d3feef70-1a2f-4ac2-d31c-3a99beea4c4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-5022a8a4-3d1c-40e5-bba8-fd12a1e967d9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_C</th>\n",
              "      <th>param_gamma</th>\n",
              "      <th>param_kernel</th>\n",
              "      <th>params</th>\n",
              "      <th>split0_test_score</th>\n",
              "      <th>split1_test_score</th>\n",
              "      <th>split2_test_score</th>\n",
              "      <th>split3_test_score</th>\n",
              "      <th>split4_test_score</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "      <th>split0_train_score</th>\n",
              "      <th>split1_train_score</th>\n",
              "      <th>split2_train_score</th>\n",
              "      <th>split3_train_score</th>\n",
              "      <th>split4_train_score</th>\n",
              "      <th>mean_train_score</th>\n",
              "      <th>std_train_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.541840</td>\n",
              "      <td>0.015152</td>\n",
              "      <td>0.173593</td>\n",
              "      <td>0.000958</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1</td>\n",
              "      <td>rbf</td>\n",
              "      <td>{'C': 0.1, 'gamma': 1, 'kernel': 'rbf'}</td>\n",
              "      <td>0.602484</td>\n",
              "      <td>0.605590</td>\n",
              "      <td>0.604037</td>\n",
              "      <td>0.605590</td>\n",
              "      <td>0.602484</td>\n",
              "      <td>0.604037</td>\n",
              "      <td>0.001389</td>\n",
              "      <td>46</td>\n",
              "      <td>0.605978</td>\n",
              "      <td>0.609472</td>\n",
              "      <td>0.605590</td>\n",
              "      <td>0.605590</td>\n",
              "      <td>0.606366</td>\n",
              "      <td>0.606599</td>\n",
              "      <td>0.001465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.281574</td>\n",
              "      <td>0.056805</td>\n",
              "      <td>0.018123</td>\n",
              "      <td>0.000834</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1</td>\n",
              "      <td>poly</td>\n",
              "      <td>{'C': 0.1, 'gamma': 1, 'kernel': 'poly'}</td>\n",
              "      <td>0.913043</td>\n",
              "      <td>0.905280</td>\n",
              "      <td>0.922360</td>\n",
              "      <td>0.895963</td>\n",
              "      <td>0.920807</td>\n",
              "      <td>0.911491</td>\n",
              "      <td>0.009870</td>\n",
              "      <td>10</td>\n",
              "      <td>0.995730</td>\n",
              "      <td>0.996118</td>\n",
              "      <td>0.994565</td>\n",
              "      <td>0.995342</td>\n",
              "      <td>0.995730</td>\n",
              "      <td>0.995497</td>\n",
              "      <td>0.000527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.217070</td>\n",
              "      <td>0.039886</td>\n",
              "      <td>0.039434</td>\n",
              "      <td>0.002477</td>\n",
              "      <td>0.1</td>\n",
              "      <td>1</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>{'C': 0.1, 'gamma': 1, 'kernel': 'sigmoid'}</td>\n",
              "      <td>0.810559</td>\n",
              "      <td>0.832298</td>\n",
              "      <td>0.787267</td>\n",
              "      <td>0.816770</td>\n",
              "      <td>0.781056</td>\n",
              "      <td>0.805590</td>\n",
              "      <td>0.018977</td>\n",
              "      <td>31</td>\n",
              "      <td>0.803571</td>\n",
              "      <td>0.802795</td>\n",
              "      <td>0.771351</td>\n",
              "      <td>0.807842</td>\n",
              "      <td>0.772516</td>\n",
              "      <td>0.791615</td>\n",
              "      <td>0.016166</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.432948</td>\n",
              "      <td>0.004254</td>\n",
              "      <td>0.136882</td>\n",
              "      <td>0.001083</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>rbf</td>\n",
              "      <td>{'C': 0.1, 'gamma': 0.1, 'kernel': 'rbf'}</td>\n",
              "      <td>0.798137</td>\n",
              "      <td>0.809006</td>\n",
              "      <td>0.818323</td>\n",
              "      <td>0.790373</td>\n",
              "      <td>0.796584</td>\n",
              "      <td>0.802484</td>\n",
              "      <td>0.009938</td>\n",
              "      <td>34</td>\n",
              "      <td>0.823758</td>\n",
              "      <td>0.823758</td>\n",
              "      <td>0.817158</td>\n",
              "      <td>0.832686</td>\n",
              "      <td>0.823758</td>\n",
              "      <td>0.824224</td>\n",
              "      <td>0.004943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.251423</td>\n",
              "      <td>0.007031</td>\n",
              "      <td>0.037359</td>\n",
              "      <td>0.002842</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>poly</td>\n",
              "      <td>{'C': 0.1, 'gamma': 0.1, 'kernel': 'poly'}</td>\n",
              "      <td>0.869565</td>\n",
              "      <td>0.869565</td>\n",
              "      <td>0.881988</td>\n",
              "      <td>0.864907</td>\n",
              "      <td>0.883540</td>\n",
              "      <td>0.873913</td>\n",
              "      <td>0.007440</td>\n",
              "      <td>23</td>\n",
              "      <td>0.917314</td>\n",
              "      <td>0.919643</td>\n",
              "      <td>0.914596</td>\n",
              "      <td>0.916925</td>\n",
              "      <td>0.920807</td>\n",
              "      <td>0.917857</td>\n",
              "      <td>0.002177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.245117</td>\n",
              "      <td>0.002783</td>\n",
              "      <td>0.043101</td>\n",
              "      <td>0.001395</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>{'C': 0.1, 'gamma': 0.1, 'kernel': 'sigmoid'}</td>\n",
              "      <td>0.849379</td>\n",
              "      <td>0.850932</td>\n",
              "      <td>0.872671</td>\n",
              "      <td>0.855590</td>\n",
              "      <td>0.854037</td>\n",
              "      <td>0.856522</td>\n",
              "      <td>0.008368</td>\n",
              "      <td>27</td>\n",
              "      <td>0.853261</td>\n",
              "      <td>0.848602</td>\n",
              "      <td>0.854037</td>\n",
              "      <td>0.845109</td>\n",
              "      <td>0.852873</td>\n",
              "      <td>0.850776</td>\n",
              "      <td>0.003407</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.308344</td>\n",
              "      <td>0.006452</td>\n",
              "      <td>0.099006</td>\n",
              "      <td>0.003409</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.01</td>\n",
              "      <td>rbf</td>\n",
              "      <td>{'C': 0.1, 'gamma': 0.01, 'kernel': 'rbf'}</td>\n",
              "      <td>0.908385</td>\n",
              "      <td>0.886646</td>\n",
              "      <td>0.909938</td>\n",
              "      <td>0.905280</td>\n",
              "      <td>0.905280</td>\n",
              "      <td>0.903106</td>\n",
              "      <td>0.008425</td>\n",
              "      <td>14</td>\n",
              "      <td>0.904891</td>\n",
              "      <td>0.906832</td>\n",
              "      <td>0.909550</td>\n",
              "      <td>0.908385</td>\n",
              "      <td>0.908385</td>\n",
              "      <td>0.907609</td>\n",
              "      <td>0.001610</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.360837</td>\n",
              "      <td>0.022515</td>\n",
              "      <td>0.065284</td>\n",
              "      <td>0.000913</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.01</td>\n",
              "      <td>poly</td>\n",
              "      <td>{'C': 0.1, 'gamma': 0.01, 'kernel': 'poly'}</td>\n",
              "      <td>0.622671</td>\n",
              "      <td>0.627329</td>\n",
              "      <td>0.633540</td>\n",
              "      <td>0.633540</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.631988</td>\n",
              "      <td>0.006804</td>\n",
              "      <td>43</td>\n",
              "      <td>0.637811</td>\n",
              "      <td>0.637422</td>\n",
              "      <td>0.636258</td>\n",
              "      <td>0.636258</td>\n",
              "      <td>0.631599</td>\n",
              "      <td>0.635870</td>\n",
              "      <td>0.002223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.309219</td>\n",
              "      <td>0.009016</td>\n",
              "      <td>0.062497</td>\n",
              "      <td>0.002073</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.01</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>{'C': 0.1, 'gamma': 0.01, 'kernel': 'sigmoid'}</td>\n",
              "      <td>0.874224</td>\n",
              "      <td>0.880435</td>\n",
              "      <td>0.891304</td>\n",
              "      <td>0.868012</td>\n",
              "      <td>0.881988</td>\n",
              "      <td>0.879193</td>\n",
              "      <td>0.007820</td>\n",
              "      <td>21</td>\n",
              "      <td>0.879658</td>\n",
              "      <td>0.877717</td>\n",
              "      <td>0.882376</td>\n",
              "      <td>0.886646</td>\n",
              "      <td>0.883929</td>\n",
              "      <td>0.882065</td>\n",
              "      <td>0.003138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.432349</td>\n",
              "      <td>0.004754</td>\n",
              "      <td>0.138570</td>\n",
              "      <td>0.004345</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>rbf</td>\n",
              "      <td>{'C': 0.1, 'gamma': 0.001, 'kernel': 'rbf'}</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.768634</td>\n",
              "      <td>0.781056</td>\n",
              "      <td>0.791925</td>\n",
              "      <td>0.793478</td>\n",
              "      <td>0.784161</td>\n",
              "      <td>0.008947</td>\n",
              "      <td>36</td>\n",
              "      <td>0.785326</td>\n",
              "      <td>0.786491</td>\n",
              "      <td>0.788043</td>\n",
              "      <td>0.790761</td>\n",
              "      <td>0.787655</td>\n",
              "      <td>0.787655</td>\n",
              "      <td>0.001821</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.373569</td>\n",
              "      <td>0.009008</td>\n",
              "      <td>0.070437</td>\n",
              "      <td>0.004610</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>poly</td>\n",
              "      <td>{'C': 0.1, 'gamma': 0.001, 'kernel': 'poly'}</td>\n",
              "      <td>0.600932</td>\n",
              "      <td>0.602484</td>\n",
              "      <td>0.602484</td>\n",
              "      <td>0.602484</td>\n",
              "      <td>0.602484</td>\n",
              "      <td>0.602174</td>\n",
              "      <td>0.000621</td>\n",
              "      <td>48</td>\n",
              "      <td>0.602873</td>\n",
              "      <td>0.602096</td>\n",
              "      <td>0.602484</td>\n",
              "      <td>0.602873</td>\n",
              "      <td>0.602484</td>\n",
              "      <td>0.602562</td>\n",
              "      <td>0.000291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.473122</td>\n",
              "      <td>0.003753</td>\n",
              "      <td>0.094834</td>\n",
              "      <td>0.000635</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>{'C': 0.1, 'gamma': 0.001, 'kernel': 'sigmoid'}</td>\n",
              "      <td>0.670807</td>\n",
              "      <td>0.658385</td>\n",
              "      <td>0.670807</td>\n",
              "      <td>0.677019</td>\n",
              "      <td>0.683230</td>\n",
              "      <td>0.672050</td>\n",
              "      <td>0.008240</td>\n",
              "      <td>42</td>\n",
              "      <td>0.673913</td>\n",
              "      <td>0.673137</td>\n",
              "      <td>0.676630</td>\n",
              "      <td>0.681289</td>\n",
              "      <td>0.669255</td>\n",
              "      <td>0.674845</td>\n",
              "      <td>0.003994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.621470</td>\n",
              "      <td>0.014350</td>\n",
              "      <td>0.167334</td>\n",
              "      <td>0.004378</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>rbf</td>\n",
              "      <td>{'C': 1, 'gamma': 1, 'kernel': 'rbf'}</td>\n",
              "      <td>0.759317</td>\n",
              "      <td>0.760870</td>\n",
              "      <td>0.770186</td>\n",
              "      <td>0.781056</td>\n",
              "      <td>0.782609</td>\n",
              "      <td>0.770807</td>\n",
              "      <td>0.009752</td>\n",
              "      <td>40</td>\n",
              "      <td>0.995342</td>\n",
              "      <td>0.994953</td>\n",
              "      <td>0.994177</td>\n",
              "      <td>0.994953</td>\n",
              "      <td>0.994177</td>\n",
              "      <td>0.994720</td>\n",
              "      <td>0.000466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.327339</td>\n",
              "      <td>0.052202</td>\n",
              "      <td>0.017095</td>\n",
              "      <td>0.000557</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>poly</td>\n",
              "      <td>{'C': 1, 'gamma': 1, 'kernel': 'poly'}</td>\n",
              "      <td>0.895963</td>\n",
              "      <td>0.903727</td>\n",
              "      <td>0.895963</td>\n",
              "      <td>0.877329</td>\n",
              "      <td>0.914596</td>\n",
              "      <td>0.897516</td>\n",
              "      <td>0.012187</td>\n",
              "      <td>18</td>\n",
              "      <td>0.997671</td>\n",
              "      <td>0.998447</td>\n",
              "      <td>0.996894</td>\n",
              "      <td>0.997283</td>\n",
              "      <td>0.997283</td>\n",
              "      <td>0.997516</td>\n",
              "      <td>0.000527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.191276</td>\n",
              "      <td>0.028902</td>\n",
              "      <td>0.035810</td>\n",
              "      <td>0.002547</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>{'C': 1, 'gamma': 1, 'kernel': 'sigmoid'}</td>\n",
              "      <td>0.793478</td>\n",
              "      <td>0.824534</td>\n",
              "      <td>0.779503</td>\n",
              "      <td>0.815217</td>\n",
              "      <td>0.802795</td>\n",
              "      <td>0.803106</td>\n",
              "      <td>0.015848</td>\n",
              "      <td>33</td>\n",
              "      <td>0.800854</td>\n",
              "      <td>0.799689</td>\n",
              "      <td>0.784938</td>\n",
              "      <td>0.804348</td>\n",
              "      <td>0.767081</td>\n",
              "      <td>0.791382</td>\n",
              "      <td>0.013849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.368711</td>\n",
              "      <td>0.005966</td>\n",
              "      <td>0.101018</td>\n",
              "      <td>0.001676</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>rbf</td>\n",
              "      <td>{'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}</td>\n",
              "      <td>0.909938</td>\n",
              "      <td>0.895963</td>\n",
              "      <td>0.914596</td>\n",
              "      <td>0.894410</td>\n",
              "      <td>0.891304</td>\n",
              "      <td>0.901242</td>\n",
              "      <td>0.009244</td>\n",
              "      <td>16</td>\n",
              "      <td>0.973991</td>\n",
              "      <td>0.975155</td>\n",
              "      <td>0.974379</td>\n",
              "      <td>0.975155</td>\n",
              "      <td>0.974767</td>\n",
              "      <td>0.974689</td>\n",
              "      <td>0.000453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.221269</td>\n",
              "      <td>0.012137</td>\n",
              "      <td>0.026471</td>\n",
              "      <td>0.001582</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>poly</td>\n",
              "      <td>{'C': 1, 'gamma': 0.1, 'kernel': 'poly'}</td>\n",
              "      <td>0.916149</td>\n",
              "      <td>0.911491</td>\n",
              "      <td>0.925466</td>\n",
              "      <td>0.911491</td>\n",
              "      <td>0.934783</td>\n",
              "      <td>0.919876</td>\n",
              "      <td>0.009033</td>\n",
              "      <td>7</td>\n",
              "      <td>0.973602</td>\n",
              "      <td>0.978261</td>\n",
              "      <td>0.971661</td>\n",
              "      <td>0.973991</td>\n",
              "      <td>0.973214</td>\n",
              "      <td>0.974146</td>\n",
              "      <td>0.002204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.163709</td>\n",
              "      <td>0.011881</td>\n",
              "      <td>0.030364</td>\n",
              "      <td>0.002431</td>\n",
              "      <td>1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>{'C': 1, 'gamma': 0.1, 'kernel': 'sigmoid'}</td>\n",
              "      <td>0.798137</td>\n",
              "      <td>0.816770</td>\n",
              "      <td>0.829193</td>\n",
              "      <td>0.815217</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.816149</td>\n",
              "      <td>0.010234</td>\n",
              "      <td>28</td>\n",
              "      <td>0.807842</td>\n",
              "      <td>0.802407</td>\n",
              "      <td>0.836568</td>\n",
              "      <td>0.808618</td>\n",
              "      <td>0.830357</td>\n",
              "      <td>0.817158</td>\n",
              "      <td>0.013626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.191845</td>\n",
              "      <td>0.004789</td>\n",
              "      <td>0.060474</td>\n",
              "      <td>0.001599</td>\n",
              "      <td>1</td>\n",
              "      <td>0.01</td>\n",
              "      <td>rbf</td>\n",
              "      <td>{'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}</td>\n",
              "      <td>0.933230</td>\n",
              "      <td>0.920807</td>\n",
              "      <td>0.944099</td>\n",
              "      <td>0.922360</td>\n",
              "      <td>0.925466</td>\n",
              "      <td>0.929193</td>\n",
              "      <td>0.008595</td>\n",
              "      <td>3</td>\n",
              "      <td>0.944488</td>\n",
              "      <td>0.942158</td>\n",
              "      <td>0.936724</td>\n",
              "      <td>0.942158</td>\n",
              "      <td>0.945264</td>\n",
              "      <td>0.942158</td>\n",
              "      <td>0.002987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.331944</td>\n",
              "      <td>0.009512</td>\n",
              "      <td>0.058524</td>\n",
              "      <td>0.001014</td>\n",
              "      <td>1</td>\n",
              "      <td>0.01</td>\n",
              "      <td>poly</td>\n",
              "      <td>{'C': 1, 'gamma': 0.01, 'kernel': 'poly'}</td>\n",
              "      <td>0.694099</td>\n",
              "      <td>0.700311</td>\n",
              "      <td>0.701863</td>\n",
              "      <td>0.706522</td>\n",
              "      <td>0.711180</td>\n",
              "      <td>0.702795</td>\n",
              "      <td>0.005777</td>\n",
              "      <td>41</td>\n",
              "      <td>0.717391</td>\n",
              "      <td>0.716227</td>\n",
              "      <td>0.713509</td>\n",
              "      <td>0.715450</td>\n",
              "      <td>0.713509</td>\n",
              "      <td>0.715217</td>\n",
              "      <td>0.001525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.194221</td>\n",
              "      <td>0.009390</td>\n",
              "      <td>0.041491</td>\n",
              "      <td>0.003078</td>\n",
              "      <td>1</td>\n",
              "      <td>0.01</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>{'C': 1, 'gamma': 0.01, 'kernel': 'sigmoid'}</td>\n",
              "      <td>0.916149</td>\n",
              "      <td>0.899068</td>\n",
              "      <td>0.917702</td>\n",
              "      <td>0.902174</td>\n",
              "      <td>0.916149</td>\n",
              "      <td>0.910248</td>\n",
              "      <td>0.007942</td>\n",
              "      <td>12</td>\n",
              "      <td>0.913043</td>\n",
              "      <td>0.908385</td>\n",
              "      <td>0.911102</td>\n",
              "      <td>0.911879</td>\n",
              "      <td>0.907997</td>\n",
              "      <td>0.910481</td>\n",
              "      <td>0.001973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.266005</td>\n",
              "      <td>0.005355</td>\n",
              "      <td>0.081745</td>\n",
              "      <td>0.003795</td>\n",
              "      <td>1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>rbf</td>\n",
              "      <td>{'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}</td>\n",
              "      <td>0.909938</td>\n",
              "      <td>0.894410</td>\n",
              "      <td>0.902174</td>\n",
              "      <td>0.899068</td>\n",
              "      <td>0.909938</td>\n",
              "      <td>0.903106</td>\n",
              "      <td>0.006102</td>\n",
              "      <td>15</td>\n",
              "      <td>0.907220</td>\n",
              "      <td>0.910326</td>\n",
              "      <td>0.905668</td>\n",
              "      <td>0.909161</td>\n",
              "      <td>0.905280</td>\n",
              "      <td>0.907531</td>\n",
              "      <td>0.001955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.379404</td>\n",
              "      <td>0.007924</td>\n",
              "      <td>0.070854</td>\n",
              "      <td>0.003952</td>\n",
              "      <td>1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>poly</td>\n",
              "      <td>{'C': 1, 'gamma': 0.001, 'kernel': 'poly'}</td>\n",
              "      <td>0.600932</td>\n",
              "      <td>0.607143</td>\n",
              "      <td>0.604037</td>\n",
              "      <td>0.602484</td>\n",
              "      <td>0.604037</td>\n",
              "      <td>0.603727</td>\n",
              "      <td>0.002060</td>\n",
              "      <td>47</td>\n",
              "      <td>0.605978</td>\n",
              "      <td>0.604037</td>\n",
              "      <td>0.605202</td>\n",
              "      <td>0.605590</td>\n",
              "      <td>0.605202</td>\n",
              "      <td>0.605202</td>\n",
              "      <td>0.000650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.306359</td>\n",
              "      <td>0.004887</td>\n",
              "      <td>0.062199</td>\n",
              "      <td>0.001566</td>\n",
              "      <td>1</td>\n",
              "      <td>0.001</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>{'C': 1, 'gamma': 0.001, 'kernel': 'sigmoid'}</td>\n",
              "      <td>0.875776</td>\n",
              "      <td>0.880435</td>\n",
              "      <td>0.889752</td>\n",
              "      <td>0.868012</td>\n",
              "      <td>0.885093</td>\n",
              "      <td>0.879814</td>\n",
              "      <td>0.007518</td>\n",
              "      <td>20</td>\n",
              "      <td>0.880047</td>\n",
              "      <td>0.879270</td>\n",
              "      <td>0.882376</td>\n",
              "      <td>0.888587</td>\n",
              "      <td>0.885481</td>\n",
              "      <td>0.883152</td>\n",
              "      <td>0.003472</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.637883</td>\n",
              "      <td>0.018208</td>\n",
              "      <td>0.163209</td>\n",
              "      <td>0.003035</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>rbf</td>\n",
              "      <td>{'C': 10, 'gamma': 1, 'kernel': 'rbf'}</td>\n",
              "      <td>0.767081</td>\n",
              "      <td>0.763975</td>\n",
              "      <td>0.774845</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.788820</td>\n",
              "      <td>0.776087</td>\n",
              "      <td>0.009840</td>\n",
              "      <td>38</td>\n",
              "      <td>0.998059</td>\n",
              "      <td>0.998447</td>\n",
              "      <td>0.997283</td>\n",
              "      <td>0.997671</td>\n",
              "      <td>0.996894</td>\n",
              "      <td>0.997671</td>\n",
              "      <td>0.000549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.340717</td>\n",
              "      <td>0.063521</td>\n",
              "      <td>0.016043</td>\n",
              "      <td>0.000503</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>poly</td>\n",
              "      <td>{'C': 10, 'gamma': 1, 'kernel': 'poly'}</td>\n",
              "      <td>0.881988</td>\n",
              "      <td>0.900621</td>\n",
              "      <td>0.886646</td>\n",
              "      <td>0.871118</td>\n",
              "      <td>0.913043</td>\n",
              "      <td>0.890683</td>\n",
              "      <td>0.014652</td>\n",
              "      <td>19</td>\n",
              "      <td>0.998059</td>\n",
              "      <td>0.998447</td>\n",
              "      <td>0.997283</td>\n",
              "      <td>0.997671</td>\n",
              "      <td>0.997283</td>\n",
              "      <td>0.997748</td>\n",
              "      <td>0.000453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.199889</td>\n",
              "      <td>0.040362</td>\n",
              "      <td>0.035519</td>\n",
              "      <td>0.005733</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>{'C': 10, 'gamma': 1, 'kernel': 'sigmoid'}</td>\n",
              "      <td>0.799689</td>\n",
              "      <td>0.826087</td>\n",
              "      <td>0.782609</td>\n",
              "      <td>0.819876</td>\n",
              "      <td>0.770186</td>\n",
              "      <td>0.799689</td>\n",
              "      <td>0.021291</td>\n",
              "      <td>35</td>\n",
              "      <td>0.801242</td>\n",
              "      <td>0.800854</td>\n",
              "      <td>0.773680</td>\n",
              "      <td>0.803571</td>\n",
              "      <td>0.767081</td>\n",
              "      <td>0.789286</td>\n",
              "      <td>0.015604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.505589</td>\n",
              "      <td>0.004680</td>\n",
              "      <td>0.096678</td>\n",
              "      <td>0.002425</td>\n",
              "      <td>10</td>\n",
              "      <td>0.1</td>\n",
              "      <td>rbf</td>\n",
              "      <td>{'C': 10, 'gamma': 0.1, 'kernel': 'rbf'}</td>\n",
              "      <td>0.909938</td>\n",
              "      <td>0.906832</td>\n",
              "      <td>0.913043</td>\n",
              "      <td>0.891304</td>\n",
              "      <td>0.913043</td>\n",
              "      <td>0.906832</td>\n",
              "      <td>0.008098</td>\n",
              "      <td>13</td>\n",
              "      <td>0.993401</td>\n",
              "      <td>0.993012</td>\n",
              "      <td>0.991848</td>\n",
              "      <td>0.991848</td>\n",
              "      <td>0.993012</td>\n",
              "      <td>0.992624</td>\n",
              "      <td>0.000650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.236353</td>\n",
              "      <td>0.017688</td>\n",
              "      <td>0.021127</td>\n",
              "      <td>0.001378</td>\n",
              "      <td>10</td>\n",
              "      <td>0.1</td>\n",
              "      <td>poly</td>\n",
              "      <td>{'C': 10, 'gamma': 0.1, 'kernel': 'poly'}</td>\n",
              "      <td>0.927019</td>\n",
              "      <td>0.923913</td>\n",
              "      <td>0.920807</td>\n",
              "      <td>0.892857</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.918634</td>\n",
              "      <td>0.013161</td>\n",
              "      <td>8</td>\n",
              "      <td>0.991460</td>\n",
              "      <td>0.991071</td>\n",
              "      <td>0.990295</td>\n",
              "      <td>0.991848</td>\n",
              "      <td>0.991848</td>\n",
              "      <td>0.991304</td>\n",
              "      <td>0.000581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.163400</td>\n",
              "      <td>0.012751</td>\n",
              "      <td>0.028365</td>\n",
              "      <td>0.001651</td>\n",
              "      <td>10</td>\n",
              "      <td>0.1</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>{'C': 10, 'gamma': 0.1, 'kernel': 'sigmoid'}</td>\n",
              "      <td>0.782609</td>\n",
              "      <td>0.813665</td>\n",
              "      <td>0.826087</td>\n",
              "      <td>0.807453</td>\n",
              "      <td>0.813665</td>\n",
              "      <td>0.808696</td>\n",
              "      <td>0.014380</td>\n",
              "      <td>29</td>\n",
              "      <td>0.811724</td>\n",
              "      <td>0.801630</td>\n",
              "      <td>0.836180</td>\n",
              "      <td>0.808618</td>\n",
              "      <td>0.827252</td>\n",
              "      <td>0.817081</td>\n",
              "      <td>0.012706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.165700</td>\n",
              "      <td>0.004287</td>\n",
              "      <td>0.044628</td>\n",
              "      <td>0.002258</td>\n",
              "      <td>10</td>\n",
              "      <td>0.01</td>\n",
              "      <td>rbf</td>\n",
              "      <td>{'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}</td>\n",
              "      <td>0.934783</td>\n",
              "      <td>0.931677</td>\n",
              "      <td>0.939441</td>\n",
              "      <td>0.931677</td>\n",
              "      <td>0.930124</td>\n",
              "      <td>0.933540</td>\n",
              "      <td>0.003316</td>\n",
              "      <td>1</td>\n",
              "      <td>0.965062</td>\n",
              "      <td>0.966227</td>\n",
              "      <td>0.958851</td>\n",
              "      <td>0.967003</td>\n",
              "      <td>0.963898</td>\n",
              "      <td>0.964208</td>\n",
              "      <td>0.002878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.295159</td>\n",
              "      <td>0.006558</td>\n",
              "      <td>0.046574</td>\n",
              "      <td>0.000981</td>\n",
              "      <td>10</td>\n",
              "      <td>0.01</td>\n",
              "      <td>poly</td>\n",
              "      <td>{'C': 10, 'gamma': 0.01, 'kernel': 'poly'}</td>\n",
              "      <td>0.796584</td>\n",
              "      <td>0.773292</td>\n",
              "      <td>0.776398</td>\n",
              "      <td>0.773292</td>\n",
              "      <td>0.798137</td>\n",
              "      <td>0.783540</td>\n",
              "      <td>0.011351</td>\n",
              "      <td>37</td>\n",
              "      <td>0.802407</td>\n",
              "      <td>0.811335</td>\n",
              "      <td>0.807453</td>\n",
              "      <td>0.812112</td>\n",
              "      <td>0.803183</td>\n",
              "      <td>0.807298</td>\n",
              "      <td>0.004009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.124783</td>\n",
              "      <td>0.011816</td>\n",
              "      <td>0.021603</td>\n",
              "      <td>0.001285</td>\n",
              "      <td>10</td>\n",
              "      <td>0.01</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>{'C': 10, 'gamma': 0.01, 'kernel': 'sigmoid'}</td>\n",
              "      <td>0.858696</td>\n",
              "      <td>0.869565</td>\n",
              "      <td>0.885093</td>\n",
              "      <td>0.881988</td>\n",
              "      <td>0.897516</td>\n",
              "      <td>0.878571</td>\n",
              "      <td>0.013336</td>\n",
              "      <td>22</td>\n",
              "      <td>0.881599</td>\n",
              "      <td>0.876553</td>\n",
              "      <td>0.885093</td>\n",
              "      <td>0.890140</td>\n",
              "      <td>0.879270</td>\n",
              "      <td>0.882531</td>\n",
              "      <td>0.004726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.182204</td>\n",
              "      <td>0.001750</td>\n",
              "      <td>0.054642</td>\n",
              "      <td>0.004292</td>\n",
              "      <td>10</td>\n",
              "      <td>0.001</td>\n",
              "      <td>rbf</td>\n",
              "      <td>{'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}</td>\n",
              "      <td>0.931677</td>\n",
              "      <td>0.916149</td>\n",
              "      <td>0.936335</td>\n",
              "      <td>0.923913</td>\n",
              "      <td>0.925466</td>\n",
              "      <td>0.926708</td>\n",
              "      <td>0.006903</td>\n",
              "      <td>5</td>\n",
              "      <td>0.931289</td>\n",
              "      <td>0.934783</td>\n",
              "      <td>0.934006</td>\n",
              "      <td>0.935947</td>\n",
              "      <td>0.934006</td>\n",
              "      <td>0.934006</td>\n",
              "      <td>0.001533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.380593</td>\n",
              "      <td>0.018068</td>\n",
              "      <td>0.067107</td>\n",
              "      <td>0.000973</td>\n",
              "      <td>10</td>\n",
              "      <td>0.001</td>\n",
              "      <td>poly</td>\n",
              "      <td>{'C': 10, 'gamma': 0.001, 'kernel': 'poly'}</td>\n",
              "      <td>0.605590</td>\n",
              "      <td>0.610248</td>\n",
              "      <td>0.613354</td>\n",
              "      <td>0.605590</td>\n",
              "      <td>0.605590</td>\n",
              "      <td>0.608075</td>\n",
              "      <td>0.003197</td>\n",
              "      <td>45</td>\n",
              "      <td>0.611025</td>\n",
              "      <td>0.611025</td>\n",
              "      <td>0.609860</td>\n",
              "      <td>0.610637</td>\n",
              "      <td>0.611025</td>\n",
              "      <td>0.610714</td>\n",
              "      <td>0.000453</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.205424</td>\n",
              "      <td>0.009960</td>\n",
              "      <td>0.040771</td>\n",
              "      <td>0.003565</td>\n",
              "      <td>10</td>\n",
              "      <td>0.001</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>{'C': 10, 'gamma': 0.001, 'kernel': 'sigmoid'}</td>\n",
              "      <td>0.925466</td>\n",
              "      <td>0.906832</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.909938</td>\n",
              "      <td>0.922360</td>\n",
              "      <td>0.918634</td>\n",
              "      <td>0.008651</td>\n",
              "      <td>8</td>\n",
              "      <td>0.921196</td>\n",
              "      <td>0.922748</td>\n",
              "      <td>0.923525</td>\n",
              "      <td>0.925078</td>\n",
              "      <td>0.921972</td>\n",
              "      <td>0.922904</td>\n",
              "      <td>0.001336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>0.614118</td>\n",
              "      <td>0.021182</td>\n",
              "      <td>0.164017</td>\n",
              "      <td>0.005369</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>rbf</td>\n",
              "      <td>{'C': 100, 'gamma': 1, 'kernel': 'rbf'}</td>\n",
              "      <td>0.763975</td>\n",
              "      <td>0.765528</td>\n",
              "      <td>0.767081</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.782609</td>\n",
              "      <td>0.772981</td>\n",
              "      <td>0.009234</td>\n",
              "      <td>39</td>\n",
              "      <td>0.998447</td>\n",
              "      <td>0.998447</td>\n",
              "      <td>0.998059</td>\n",
              "      <td>0.998059</td>\n",
              "      <td>0.997283</td>\n",
              "      <td>0.998059</td>\n",
              "      <td>0.000425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>0.445556</td>\n",
              "      <td>0.082415</td>\n",
              "      <td>0.014761</td>\n",
              "      <td>0.001184</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>poly</td>\n",
              "      <td>{'C': 100, 'gamma': 1, 'kernel': 'poly'}</td>\n",
              "      <td>0.875776</td>\n",
              "      <td>0.863354</td>\n",
              "      <td>0.866460</td>\n",
              "      <td>0.843168</td>\n",
              "      <td>0.872671</td>\n",
              "      <td>0.864286</td>\n",
              "      <td>0.011436</td>\n",
              "      <td>25</td>\n",
              "      <td>0.998835</td>\n",
              "      <td>0.999224</td>\n",
              "      <td>0.998835</td>\n",
              "      <td>0.999224</td>\n",
              "      <td>0.998447</td>\n",
              "      <td>0.998913</td>\n",
              "      <td>0.000291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.197542</td>\n",
              "      <td>0.034063</td>\n",
              "      <td>0.032972</td>\n",
              "      <td>0.001835</td>\n",
              "      <td>100</td>\n",
              "      <td>1</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>{'C': 100, 'gamma': 1, 'kernel': 'sigmoid'}</td>\n",
              "      <td>0.798137</td>\n",
              "      <td>0.824534</td>\n",
              "      <td>0.781056</td>\n",
              "      <td>0.815217</td>\n",
              "      <td>0.799689</td>\n",
              "      <td>0.803727</td>\n",
              "      <td>0.015010</td>\n",
              "      <td>32</td>\n",
              "      <td>0.802019</td>\n",
              "      <td>0.801242</td>\n",
              "      <td>0.774845</td>\n",
              "      <td>0.805124</td>\n",
              "      <td>0.779115</td>\n",
              "      <td>0.792469</td>\n",
              "      <td>0.012785</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.394633</td>\n",
              "      <td>0.011301</td>\n",
              "      <td>0.090510</td>\n",
              "      <td>0.001228</td>\n",
              "      <td>100</td>\n",
              "      <td>0.1</td>\n",
              "      <td>rbf</td>\n",
              "      <td>{'C': 100, 'gamma': 0.1, 'kernel': 'rbf'}</td>\n",
              "      <td>0.903727</td>\n",
              "      <td>0.897516</td>\n",
              "      <td>0.909938</td>\n",
              "      <td>0.875776</td>\n",
              "      <td>0.914596</td>\n",
              "      <td>0.900311</td>\n",
              "      <td>0.013551</td>\n",
              "      <td>17</td>\n",
              "      <td>0.995730</td>\n",
              "      <td>0.996506</td>\n",
              "      <td>0.994953</td>\n",
              "      <td>0.995730</td>\n",
              "      <td>0.995730</td>\n",
              "      <td>0.995730</td>\n",
              "      <td>0.000491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>0.298018</td>\n",
              "      <td>0.079937</td>\n",
              "      <td>0.018791</td>\n",
              "      <td>0.000719</td>\n",
              "      <td>100</td>\n",
              "      <td>0.1</td>\n",
              "      <td>poly</td>\n",
              "      <td>{'C': 100, 'gamma': 0.1, 'kernel': 'poly'}</td>\n",
              "      <td>0.913043</td>\n",
              "      <td>0.905280</td>\n",
              "      <td>0.922360</td>\n",
              "      <td>0.895963</td>\n",
              "      <td>0.920807</td>\n",
              "      <td>0.911491</td>\n",
              "      <td>0.009870</td>\n",
              "      <td>10</td>\n",
              "      <td>0.995730</td>\n",
              "      <td>0.996118</td>\n",
              "      <td>0.994565</td>\n",
              "      <td>0.995342</td>\n",
              "      <td>0.995730</td>\n",
              "      <td>0.995497</td>\n",
              "      <td>0.000527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>0.160957</td>\n",
              "      <td>0.015066</td>\n",
              "      <td>0.028094</td>\n",
              "      <td>0.002163</td>\n",
              "      <td>100</td>\n",
              "      <td>0.1</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>{'C': 100, 'gamma': 0.1, 'kernel': 'sigmoid'}</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.810559</td>\n",
              "      <td>0.821429</td>\n",
              "      <td>0.810559</td>\n",
              "      <td>0.813665</td>\n",
              "      <td>0.808385</td>\n",
              "      <td>0.012012</td>\n",
              "      <td>30</td>\n",
              "      <td>0.808230</td>\n",
              "      <td>0.803960</td>\n",
              "      <td>0.838509</td>\n",
              "      <td>0.804348</td>\n",
              "      <td>0.827640</td>\n",
              "      <td>0.816537</td>\n",
              "      <td>0.014013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>0.200471</td>\n",
              "      <td>0.008072</td>\n",
              "      <td>0.039054</td>\n",
              "      <td>0.000427</td>\n",
              "      <td>100</td>\n",
              "      <td>0.01</td>\n",
              "      <td>rbf</td>\n",
              "      <td>{'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}</td>\n",
              "      <td>0.931677</td>\n",
              "      <td>0.922360</td>\n",
              "      <td>0.934783</td>\n",
              "      <td>0.919255</td>\n",
              "      <td>0.933230</td>\n",
              "      <td>0.928261</td>\n",
              "      <td>0.006242</td>\n",
              "      <td>4</td>\n",
              "      <td>0.981366</td>\n",
              "      <td>0.982531</td>\n",
              "      <td>0.980590</td>\n",
              "      <td>0.982919</td>\n",
              "      <td>0.984084</td>\n",
              "      <td>0.982298</td>\n",
              "      <td>0.001218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.249146</td>\n",
              "      <td>0.007864</td>\n",
              "      <td>0.036932</td>\n",
              "      <td>0.002839</td>\n",
              "      <td>100</td>\n",
              "      <td>0.01</td>\n",
              "      <td>poly</td>\n",
              "      <td>{'C': 100, 'gamma': 0.01, 'kernel': 'poly'}</td>\n",
              "      <td>0.869565</td>\n",
              "      <td>0.869565</td>\n",
              "      <td>0.881988</td>\n",
              "      <td>0.864907</td>\n",
              "      <td>0.883540</td>\n",
              "      <td>0.873913</td>\n",
              "      <td>0.007440</td>\n",
              "      <td>23</td>\n",
              "      <td>0.917314</td>\n",
              "      <td>0.919643</td>\n",
              "      <td>0.914596</td>\n",
              "      <td>0.916925</td>\n",
              "      <td>0.920807</td>\n",
              "      <td>0.917857</td>\n",
              "      <td>0.002177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>0.106310</td>\n",
              "      <td>0.006403</td>\n",
              "      <td>0.019417</td>\n",
              "      <td>0.001642</td>\n",
              "      <td>100</td>\n",
              "      <td>0.01</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>{'C': 100, 'gamma': 0.01, 'kernel': 'sigmoid'}</td>\n",
              "      <td>0.855590</td>\n",
              "      <td>0.855590</td>\n",
              "      <td>0.866460</td>\n",
              "      <td>0.864907</td>\n",
              "      <td>0.878882</td>\n",
              "      <td>0.864286</td>\n",
              "      <td>0.008595</td>\n",
              "      <td>25</td>\n",
              "      <td>0.876553</td>\n",
              "      <td>0.868012</td>\n",
              "      <td>0.878106</td>\n",
              "      <td>0.873447</td>\n",
              "      <td>0.873447</td>\n",
              "      <td>0.873913</td>\n",
              "      <td>0.003458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0.171537</td>\n",
              "      <td>0.003103</td>\n",
              "      <td>0.040946</td>\n",
              "      <td>0.002545</td>\n",
              "      <td>100</td>\n",
              "      <td>0.001</td>\n",
              "      <td>rbf</td>\n",
              "      <td>{'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}</td>\n",
              "      <td>0.939441</td>\n",
              "      <td>0.927019</td>\n",
              "      <td>0.936335</td>\n",
              "      <td>0.931677</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.932609</td>\n",
              "      <td>0.004669</td>\n",
              "      <td>2</td>\n",
              "      <td>0.948758</td>\n",
              "      <td>0.945264</td>\n",
              "      <td>0.945264</td>\n",
              "      <td>0.947205</td>\n",
              "      <td>0.949146</td>\n",
              "      <td>0.947127</td>\n",
              "      <td>0.001654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>0.367587</td>\n",
              "      <td>0.021300</td>\n",
              "      <td>0.066767</td>\n",
              "      <td>0.003764</td>\n",
              "      <td>100</td>\n",
              "      <td>0.001</td>\n",
              "      <td>poly</td>\n",
              "      <td>{'C': 100, 'gamma': 0.001, 'kernel': 'poly'}</td>\n",
              "      <td>0.622671</td>\n",
              "      <td>0.627329</td>\n",
              "      <td>0.633540</td>\n",
              "      <td>0.633540</td>\n",
              "      <td>0.642857</td>\n",
              "      <td>0.631988</td>\n",
              "      <td>0.006804</td>\n",
              "      <td>43</td>\n",
              "      <td>0.637811</td>\n",
              "      <td>0.637422</td>\n",
              "      <td>0.636258</td>\n",
              "      <td>0.636258</td>\n",
              "      <td>0.631599</td>\n",
              "      <td>0.635870</td>\n",
              "      <td>0.002223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.172550</td>\n",
              "      <td>0.013259</td>\n",
              "      <td>0.026770</td>\n",
              "      <td>0.002378</td>\n",
              "      <td>100</td>\n",
              "      <td>0.001</td>\n",
              "      <td>sigmoid</td>\n",
              "      <td>{'C': 100, 'gamma': 0.001, 'kernel': 'sigmoid'}</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.922360</td>\n",
              "      <td>0.919255</td>\n",
              "      <td>0.903727</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.920497</td>\n",
              "      <td>0.009129</td>\n",
              "      <td>6</td>\n",
              "      <td>0.929736</td>\n",
              "      <td>0.928571</td>\n",
              "      <td>0.926242</td>\n",
              "      <td>0.926630</td>\n",
              "      <td>0.930124</td>\n",
              "      <td>0.928261</td>\n",
              "      <td>0.001580</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5022a8a4-3d1c-40e5-bba8-fd12a1e967d9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5022a8a4-3d1c-40e5-bba8-fd12a1e967d9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5022a8a4-3d1c-40e5-bba8-fd12a1e967d9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    mean_fit_time  std_fit_time  ...  mean_train_score  std_train_score\n",
              "0        0.541840      0.015152  ...          0.606599         0.001465\n",
              "1        0.281574      0.056805  ...          0.995497         0.000527\n",
              "2        0.217070      0.039886  ...          0.791615         0.016166\n",
              "3        0.432948      0.004254  ...          0.824224         0.004943\n",
              "4        0.251423      0.007031  ...          0.917857         0.002177\n",
              "5        0.245117      0.002783  ...          0.850776         0.003407\n",
              "6        0.308344      0.006452  ...          0.907609         0.001610\n",
              "7        0.360837      0.022515  ...          0.635870         0.002223\n",
              "8        0.309219      0.009016  ...          0.882065         0.003138\n",
              "9        0.432349      0.004754  ...          0.787655         0.001821\n",
              "10       0.373569      0.009008  ...          0.602562         0.000291\n",
              "11       0.473122      0.003753  ...          0.674845         0.003994\n",
              "12       0.621470      0.014350  ...          0.994720         0.000466\n",
              "13       0.327339      0.052202  ...          0.997516         0.000527\n",
              "14       0.191276      0.028902  ...          0.791382         0.013849\n",
              "15       0.368711      0.005966  ...          0.974689         0.000453\n",
              "16       0.221269      0.012137  ...          0.974146         0.002204\n",
              "17       0.163709      0.011881  ...          0.817158         0.013626\n",
              "18       0.191845      0.004789  ...          0.942158         0.002987\n",
              "19       0.331944      0.009512  ...          0.715217         0.001525\n",
              "20       0.194221      0.009390  ...          0.910481         0.001973\n",
              "21       0.266005      0.005355  ...          0.907531         0.001955\n",
              "22       0.379404      0.007924  ...          0.605202         0.000650\n",
              "23       0.306359      0.004887  ...          0.883152         0.003472\n",
              "24       0.637883      0.018208  ...          0.997671         0.000549\n",
              "25       0.340717      0.063521  ...          0.997748         0.000453\n",
              "26       0.199889      0.040362  ...          0.789286         0.015604\n",
              "27       0.505589      0.004680  ...          0.992624         0.000650\n",
              "28       0.236353      0.017688  ...          0.991304         0.000581\n",
              "29       0.163400      0.012751  ...          0.817081         0.012706\n",
              "30       0.165700      0.004287  ...          0.964208         0.002878\n",
              "31       0.295159      0.006558  ...          0.807298         0.004009\n",
              "32       0.124783      0.011816  ...          0.882531         0.004726\n",
              "33       0.182204      0.001750  ...          0.934006         0.001533\n",
              "34       0.380593      0.018068  ...          0.610714         0.000453\n",
              "35       0.205424      0.009960  ...          0.922904         0.001336\n",
              "36       0.614118      0.021182  ...          0.998059         0.000425\n",
              "37       0.445556      0.082415  ...          0.998913         0.000291\n",
              "38       0.197542      0.034063  ...          0.792469         0.012785\n",
              "39       0.394633      0.011301  ...          0.995730         0.000491\n",
              "40       0.298018      0.079937  ...          0.995497         0.000527\n",
              "41       0.160957      0.015066  ...          0.816537         0.014013\n",
              "42       0.200471      0.008072  ...          0.982298         0.001218\n",
              "43       0.249146      0.007864  ...          0.917857         0.002177\n",
              "44       0.106310      0.006403  ...          0.873913         0.003458\n",
              "45       0.171537      0.003103  ...          0.947127         0.001654\n",
              "46       0.367587      0.021300  ...          0.635870         0.002223\n",
              "47       0.172550      0.013259  ...          0.928261         0.001580\n",
              "\n",
              "[48 rows x 23 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "# cv results into a dataframe\n",
        "cv_results = pd.DataFrame(grid.cv_results_)\n",
        "\n",
        "# print cv_result\n",
        "cv_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvkiOHW20X9H"
      },
      "source": [
        "- **High values of gamma** lead to **overfitting** (especially at high values of C); note that the training accuracy at gamma=0.01 and C=1000 reaches almost 99% \n",
        "- The **training score increases with higher gamma**, though the **test scores are comparable** (at sufficiently high cost, i.e. C > 10)\n",
        "- The least amount of overfitting (i.e. difference between train and test accuracy) occurs at low gamma, i.e. a quite *simple non-linear model*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "XFN1ZsW90X9H",
        "outputId": "90da2c1b-2b39-41fd-8d58-d66da26cf52a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The best test score is 0.9335403726708075 corresponding to hyperparameters {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n"
          ]
        }
      ],
      "source": [
        "# get the optimal accuracy score and hyperparameters\n",
        "best_score = grid.best_score_\n",
        "\n",
        "# get the optimal hyperparameters\n",
        "best_hyperparams = grid.best_params_\n",
        "\n",
        "print(\"The best test score is {0} corresponding to hyperparameters {1}\".format(best_score, best_hyperparams))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kw3JBvcq0X9I"
      },
      "source": [
        "Though sklearn suggests the optimal scores mentioned above (gamma=0.001, C=100, kernel=rbf), one could argue that it is better to choose a simpler, more non-linear model with gamma=0.0001. This is because the optimal values mentioned here are calculated based on the average test accuracy (but not considering subjective parameters such as model complexity).\n",
        "\n",
        "We can achieve comparable average test accuracy (~92.5%) with gamma=0.0001 as well, though we'll have to increase the cost C for that. So to achieve high accuracy, there's a tradeoff between:\n",
        "- High gamma (i.e. high non-linearity) and average value of C\n",
        "- Low gamma (i.e. less non-linearity) and high value of C\n",
        "\n",
        "We argue that the model will be simpler if it has as less non-linearity as possible, so we choose gamma=0.0001 and a high C=100."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twjsmr3M0X9I"
      },
      "source": [
        "### Building and Evaluating the Final Model\n",
        "\n",
        "Let's now build and evaluate the final model, i.e. the model with highest test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "95cP9q-f0X9I",
        "outputId": "0bbd7c59-2016-478e-a468-8a6dfdd4f3f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[812  37]\n",
            " [ 53 479]] \n",
            "\n",
            "accuracy 0.9348298334540188\n",
            "precision 0.9282945736434108\n",
            "sensitivity/recall 0.900375939849624\n"
          ]
        }
      ],
      "source": [
        "# model with optimal hyperparameters as obtained above\n",
        "model = SVC(C=10, gamma=0.01, kernel='rbf')\n",
        "\n",
        "# fit model on training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# print confusion metrics\n",
        "print(metrics.confusion_matrix(y_test, y_pred), \"\\n\")\n",
        "\n",
        "# print accuracy\n",
        "print(\"accuracy\", metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "# print precision\n",
        "print(\"precision\", metrics.precision_score(y_test, y_pred))\n",
        "\n",
        "# print sensitivity\n",
        "print(\"sensitivity/recall\", metrics.recall_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DArpLS7S0X9I"
      },
      "source": [
        "Hurray! we improved our model by reduced type1 and type 2 errors after hyperparameter tuning.\n",
        "\n",
        "Note: Don't worry if your result varies plus minus 1-2%. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDq7Viqo0X9I"
      },
      "source": [
        "---------------------\n",
        "# Party time ! Congrats..you have completed your SVM milestone challenge\n",
        "\n",
        "----------------\n",
        "\n",
        "# FeedBack Time\n",
        "\n",
        "We hope youâ€™ve enjoyed this course so far. Weâ€™re committed to help you use \"AI for All\" course to its full potential, so that you have a great learning experience. And thatâ€™s why we need your help in form of a feedback here.\n",
        "\n",
        "Please fill this feedback form https://docs.google.com/forms/d/e/1FAIpQLSfjBmH0yJSSA34IhSVx4h2eDMgOAeG4Dk-yHid__NMTk3Hq5g/viewform"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "SVM_CloudyML.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "e4TF8h-_0X9D"
      ],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}